#!/usr/bin/env python3
"""
ë‚´ë¶€ë§í¬ ìë™ ìƒì„± ìœ í‹¸ë¦¬í‹°
- FAISS ë²¡í„° ì €ì¥ì†Œ ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰
- ì½˜í…ì¸  ê°„ ìë™ ë‚´ë¶€ë§í¬ ìƒì„±
- ì•µì»¤ í…ìŠ¤íŠ¸ ìµœì í™”
"""

import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import re
import random

from .content_storage import ContentStorage

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)


@dataclass
class InternalLink:
    """ë‚´ë¶€ë§í¬ ë°ì´í„° í´ë˜ìŠ¤"""

    anchor_text: str
    target_url: str
    target_title: str
    target_post_id: str
    similarity_score: float
    keyword_type: str  # 'keyword', 'lsi', 'longtail'


class InternalLinkBuilder:
    """ë‚´ë¶€ë§í¬ ìë™ ìƒì„± í´ë˜ìŠ¤"""

    def __init__(self, content_storage: ContentStorage):
        """
        ë‚´ë¶€ë§í¬ ë¹Œë” ì´ˆê¸°í™”

        Args:
            content_storage: ì½˜í…ì¸  ì €ì¥ì†Œ ì¸ìŠ¤í„´ìŠ¤
        """
        self.content_storage = content_storage
        logger.info("InternalLinkBuilder ì´ˆê¸°í™” ì™„ë£Œ")

    def generate_internal_links(
        self,
        current_post_id: str,
        keywords_data: Dict[str, List[str]],
        target_keyword: str,
        markdown_content: str,
        max_links: int = 3,
        min_similarity_score: float = 0.4,  # ìœ ì‚¬ë„ ì„ê³„ê°’ ì™„í™” (0.75 â†’ 0.4)
    ) -> List[InternalLink]:
        """
        í˜„ì¬ í¬ìŠ¤íŠ¸ì— ëŒ€í•œ ë‚´ë¶€ë§í¬ ìƒì„±

        Args:
            current_post_id: í˜„ì¬ í¬ìŠ¤íŠ¸ ID
            keywords_data: í‚¤ì›Œë“œ ë°ì´í„° (lsi_keywords, longtail_keywords)
            target_keyword: ì£¼ìš” í‚¤ì›Œë“œ
            markdown_content: ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ 
            max_links: ìµœëŒ€ ë§í¬ ìˆ˜
            min_similarity_score: ìµœì†Œ ìœ ì‚¬ë„ ì ìˆ˜

        Returns:
            ìƒì„±ëœ ë‚´ë¶€ë§í¬ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ë³¸ë¬¸ì—ì„œ ì‹¤ì œ ì‚¬ìš©ëœ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
            used_keywords = self._extract_keywords_from_content(
                markdown_content, keywords_data
            )

            if (
                not used_keywords["lsi_keywords"]
                and not used_keywords["longtail_keywords"]
            ):
                logger.warning(
                    "ë³¸ë¬¸ì—ì„œ ì‚¬ìš©ëœ í‚¤ì›Œë“œê°€ ì—†ì–´ ë‚´ë¶€ë§í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŒ"
                )
                return []

            # ê° í‚¤ì›Œë“œì— ëŒ€í•´ ìœ ì‚¬í•œ í¬ìŠ¤íŠ¸ ê²€ìƒ‰
            internal_links = []
            all_keywords = (
                [(kw, "lsi") for kw in used_keywords["lsi_keywords"]]
                + [(kw, "longtail") for kw in used_keywords["longtail_keywords"]]
                + [(target_keyword, "keyword")]
            )

            # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ê´€ë ¨ í¬ìŠ¤íŠ¸ ê²€ìƒ‰ (ì„ë² ë”© ë¶ˆí•„ìš”)
            keyword_list = [kw for kw, _ in all_keywords]
            matched_posts = self.content_storage.find_posts_by_keyword_similarity(
                target_keywords=keyword_list,
                exclude_post_id=current_post_id,
                min_keyword_match=1,  # ìµœì†Œ 1ê°œ í‚¤ì›Œë“œ ë§¤ì¹˜
            )

            # ì´ë¯¸ ì‚¬ìš©ëœ í‚¤ì›Œë“œ ì¶”ì  (ì¤‘ë³µ ë°©ì§€)
            used_anchor_texts = set()

            # ë§¤ì¹­ëœ í¬ìŠ¤íŠ¸ë“¤ë¡œ ë‚´ë¶€ë§í¬ ìƒì„±
            for post in matched_posts:
                if len(internal_links) >= max_links:
                    break

                # ì´ë¯¸ ê°™ì€ í¬ìŠ¤íŠ¸ì— ëŒ€í•œ ë§í¬ê°€ ìˆëŠ”ì§€ ì²´í¬
                if any(
                    link.target_post_id == post["post_id"] for link in internal_links
                ):
                    continue

                # ë§¤ì¹­ëœ í‚¤ì›Œë“œë“¤ ì¤‘ì—ì„œ ì²« ë²ˆì§¸ ì‚¬ìš©
                if post.get("matched_keywords"):
                    target_kw, matched_kw = post["matched_keywords"][0]

                    # ì´ë¯¸ ê°™ì€ í‚¤ì›Œë“œë¡œ ë§í¬ê°€ ìƒì„±ë˜ì—ˆëŠ”ì§€ ì²´í¬ (ì¤‘ë³µ ë°©ì§€)
                    if target_kw in used_anchor_texts:
                        continue

                    # í‚¤ì›Œë“œ íƒ€ì… ì°¾ê¸°
                    keyword_type = "keyword"
                    for kw, kw_type in all_keywords:
                        if kw == target_kw:
                            keyword_type = kw_type
                            break

                    internal_link = InternalLink(
                        anchor_text=target_kw,
                        target_url=post["url"],
                        target_title=post["title"],
                        target_post_id=post["post_id"],
                        similarity_score=post["match_score"],
                        keyword_type=keyword_type,
                    )

                    internal_links.append(internal_link)
                    used_anchor_texts.add(target_kw)  # ì‚¬ìš©ëœ í‚¤ì›Œë“œ ê¸°ë¡

            # ìœ ì‚¬ë„ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            internal_links.sort(key=lambda x: x.similarity_score, reverse=True)

            logger.info(
                f"ë‚´ë¶€ë§í¬ {len(internal_links)}ê°œ ìƒì„± (í¬ìŠ¤íŠ¸ ID: {current_post_id})"
            )
            return internal_links[:max_links]

        except Exception as e:
            logger.error(f"ë‚´ë¶€ë§í¬ ìƒì„± ì‹¤íŒ¨ (í¬ìŠ¤íŠ¸ ID: {current_post_id}): {e}")
            return []

    def _extract_keywords_from_content(
        self, content: str, keywords_data: Dict[str, List[str]]
    ) -> Dict[str, List[str]]:
        """ë³¸ë¬¸ ì½˜í…ì¸ ì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©ëœ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ"""
        # H2 íƒœê·¸ ì´í›„ì˜ ë³¸ë¬¸ë§Œ ì¶”ì¶œ
        lines = content.split("\n")
        main_content = []
        main_content_started = False

        for line in lines:
            if line.startswith("## ") and not main_content_started:
                main_content_started = True
                continue
            if main_content_started:
                main_content.append(line)

        # ì‹¤ì œ ë³¸ë¬¸ í…ìŠ¤íŠ¸
        main_text = "\n".join(main_content)

        # ì‹¤ì œ ë³¸ë¬¸ì— ì¡´ì¬í•˜ëŠ” í‚¤ì›Œë“œë§Œ í•„í„°ë§
        used_keywords = {"lsi_keywords": [], "longtail_keywords": []}

        # LSI í‚¤ì›Œë“œ í™•ì¸
        for keyword in keywords_data.get("lsi_keywords", []):
            if keyword in main_text:
                used_keywords["lsi_keywords"].append(keyword)

        # ë¡±í…Œì¼ í‚¤ì›Œë“œ í™•ì¸
        for keyword in keywords_data.get("longtail_keywords", []):
            if keyword in main_text:
                used_keywords["longtail_keywords"].append(keyword)

        return used_keywords

    def insert_internal_links_into_markdown(
        self, markdown_content: str, internal_links: List[InternalLink]
    ) -> str:
        """ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ ì— ë‚´ë¶€ë§í¬ ì‚½ì…"""
        if not internal_links:
            return markdown_content

        try:
            content_lines = markdown_content.split("\n")
            modified_content_lines = []
            links_to_apply = internal_links.copy()

            # ì²« ë²ˆì§¸ H2 íƒœê·¸ ì´í›„ë¶€í„° ë§í¬ ì‚½ì…
            h2_found = False

            for line in content_lines:
                # ì²« ë²ˆì§¸ H2 íƒœê·¸ í™•ì¸
                if line.startswith("## ") and not h2_found:
                    h2_found = True
                    modified_content_lines.append(line)
                    continue

                # H2 íƒœê·¸ ì´í›„ì—ë§Œ ë§í¬ ì‚½ì…
                if h2_found and links_to_apply:
                    modified_line = line

                    # ì‚¬ìš© ê°€ëŠ¥í•œ ë§í¬ë“¤ì„ ìˆœíšŒí•˜ë©´ì„œ ì²« ë²ˆì§¸ ë§¤ì¹­ë˜ëŠ” ê²ƒ ì‚¬ìš©
                    for link in links_to_apply[:]:  # ë³µì‚¬ë³¸ì„ ìˆœíšŒ
                        # ì•µì»¤ í…ìŠ¤íŠ¸ê°€ ë¼ì¸ì— ìˆê³ , ì•„ì§ ë§í¬ë¡œ ë³€í™˜ë˜ì§€ ì•Šì€ ê²½ìš°
                        if (
                            link.anchor_text in modified_line
                            and f"[{link.anchor_text}]" not in modified_line
                            and not modified_line.startswith("#")
                        ):  # í—¤ë”©ì€ ì œì™¸

                            # ì²« ë²ˆì§¸ ë°œê²¬ëœ í‚¤ì›Œë“œë§Œ ë§í¬ë¡œ ë³€í™˜
                            modified_line = modified_line.replace(
                                link.anchor_text,
                                f"[{link.anchor_text}]({link.target_url})",
                                1,  # ì²« ë²ˆì§¸ ë°œê²¬ëœ ê²ƒë§Œ êµì²´
                            )

                            # ì‚¬ìš©ëœ ë§í¬ëŠ” ì œê±°í•˜ì—¬ ì¤‘ë³µ ì‚¬ìš© ë°©ì§€
                            links_to_apply.remove(link)
                            logger.info(
                                f"ë‚´ë¶€ë§í¬ ì ìš©: {link.anchor_text} â†’ {link.target_title}"
                            )
                            break  # í•œ ë¼ì¸ì—ëŠ” í•˜ë‚˜ì˜ ë§í¬ë§Œ

                    modified_content_lines.append(modified_line)
                else:
                    modified_content_lines.append(line)

            return "\n".join(modified_content_lines)

        except Exception as e:
            logger.error(f"ë‚´ë¶€ë§í¬ ì‚½ì… ì‹¤íŒ¨: {e}")
            return markdown_content

    def get_internal_links_summary(
        self, internal_links: List[InternalLink]
    ) -> Dict[str, Any]:
        """ë‚´ë¶€ë§í¬ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        if not internal_links:
            return {
                "ì´_ë§í¬_ìˆ˜": 0,
                "í‚¤ì›Œë“œ_ë§í¬_ìˆ˜": 0,
                "LSI_ë§í¬_ìˆ˜": 0,
                "ë¡±í…Œì¼_ë§í¬_ìˆ˜": 0,
                "í‰ê· _ìœ ì‚¬ë„": 0.0,
            }

        keyword_count = len([l for l in internal_links if l.keyword_type == "keyword"])
        lsi_count = len([l for l in internal_links if l.keyword_type == "lsi"])
        longtail_count = len(
            [l for l in internal_links if l.keyword_type == "longtail"]
        )
        avg_similarity = sum(l.similarity_score for l in internal_links) / len(
            internal_links
        )

        return {
            "ì´_ë§í¬_ìˆ˜": len(internal_links),
            "í‚¤ì›Œë“œ_ë§í¬_ìˆ˜": keyword_count,
            "LSI_ë§í¬_ìˆ˜": lsi_count,
            "ë¡±í…Œì¼_ë§í¬_ìˆ˜": longtail_count,
            "í‰ê· _ìœ ì‚¬ë„": round(avg_similarity, 3),
        }

    def suggest_link_opportunities(
        self,
        current_post_id: str,
        keywords_data: Dict[str, List[str]],
        target_keyword: str,
    ) -> List[Dict[str, Any]]:
        """
        ë§í¬ ê¸°íšŒ ì œì•ˆ (ë¶„ì„ìš©)

        Returns:
            ë§í¬ ê¸°íšŒ ë¦¬ìŠ¤íŠ¸ (í‚¤ì›Œë“œ, ìœ ì‚¬ í¬ìŠ¤íŠ¸, ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨)
        """
        try:
            opportunities = []
            all_keywords = (
                keywords_data.get("lsi_keywords", [])
                + keywords_data.get("longtail_keywords", [])
                + [target_keyword]
            )

            # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ê´€ë ¨ í¬ìŠ¤íŠ¸ ê²€ìƒ‰ (ì„ë² ë”© ë¶ˆí•„ìš”)
            matched_posts = self.content_storage.find_posts_by_keyword_similarity(
                target_keywords=all_keywords,
                exclude_post_id=current_post_id,
                min_keyword_match=1,  # ìµœì†Œ 1ê°œ í‚¤ì›Œë“œ ë§¤ì¹˜
            )

            # ë§¤ì¹­ëœ í¬ìŠ¤íŠ¸ë“¤ì„ ê¸°íšŒë¡œ ë³€í™˜
            for post in matched_posts:
                # ë§¤ì¹­ëœ í‚¤ì›Œë“œë“¤ ì¤‘ì—ì„œ ì²« ë²ˆì§¸ ì‚¬ìš©
                if post.get("matched_keywords"):
                    target_kw, matched_kw = post["matched_keywords"][0]
                    opportunities.append(
                        {
                            "keyword": target_kw,
                            "similar_posts_count": 1,
                            "best_match": {
                                "title": post["title"],
                                "similarity": post["match_score"],
                            },
                            "all_matches": [post],
                            "match_score": post["match_score"],
                            "matched_keyword": matched_kw,
                        }
                    )

            # ë§¤ì¹­ ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
            opportunities.sort(
                key=lambda x: x.get("match_score", 0),
                reverse=True,
            )

            return opportunities

        except Exception as e:
            logger.error(f"ë§í¬ ê¸°íšŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []


# í¸ì˜ë¥¼ ìœ„í•œ íŒ©í† ë¦¬ í•¨ìˆ˜
def create_internal_link_builder(
    content_storage: ContentStorage,
) -> InternalLinkBuilder:
    """ë‚´ë¶€ë§í¬ ë¹Œë” ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return InternalLinkBuilder(content_storage)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    from .content_storage import create_content_storage

    storage = create_content_storage()
    builder = create_internal_link_builder(storage)

    print("ğŸ“Š ë‚´ë¶€ë§í¬ ë¹Œë” í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
