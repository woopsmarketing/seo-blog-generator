#!/usr/bin/env python3
"""
ì½˜í…ì¸  ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°
- FAISS ê¸°ë°˜ ë²¡í„° ì €ì¥ì†Œ
- ì›Œë“œí”„ë ˆìŠ¤ í¬ìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„° ê´€ë¦¬
- ë‚´ë¶€ë§í¬ ìƒì„±ì„ ìœ„í•œ ìœ ì‚¬ë„ ê³„ì‚°
"""

import os
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import hashlib

import faiss
import numpy as np
from langchain_openai import OpenAIEmbeddings
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)


class ContentStorage:
    """ì½˜í…ì¸  ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, storage_dir: str = "data/content_storage"):
        """
        ì½˜í…ì¸  ì €ì¥ì†Œ ì´ˆê¸°í™”

        Args:
            storage_dir: ë²¡í„° ì €ì¥ì†Œ ë° ë©”íƒ€ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(parents=True, exist_ok=True)

        # FAISS ë²¡í„° ì €ì¥ì†Œ ê²½ë¡œ
        self.vector_store_path = self.storage_dir / "faiss_index"

        # ë©”íƒ€ë°ì´í„° ì €ì¥ ê²½ë¡œ
        self.metadata_file = self.storage_dir / "content_metadata.json"

        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",  # ë¹„ìš© íš¨ìœ¨ì ì¸ ì„ë² ë”© ëª¨ë¸
            openai_api_key=os.getenv("OPENAI_API_KEY"),
        )

        # ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ë˜ëŠ” ì´ˆê¸°í™”
        self.vector_store = self._load_or_create_vector_store()

        # ë©”íƒ€ë°ì´í„° ë¡œë“œ ë˜ëŠ” ì´ˆê¸°í™”
        self.metadata = self._load_or_create_metadata()

        logger.info(f"ContentStorage ì´ˆê¸°í™” ì™„ë£Œ: {self.storage_dir}")

    def _load_or_create_vector_store(self) -> FAISS:
        """FAISS ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ë˜ëŠ” ìƒì„±"""
        try:
            if self.vector_store_path.exists():
                # ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
                vector_store = FAISS.load_local(
                    str(self.vector_store_path),
                    self.embeddings,
                    allow_dangerous_deserialization=True,
                )
                logger.info(
                    f"ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì™„ë£Œ: {len(vector_store.docstore._dict)} ë¬¸ì„œ"
                )
                return vector_store
            else:
                # ìƒˆ ë²¡í„° ì €ì¥ì†Œ ìƒì„±
                logger.info("ìƒˆë¡œìš´ ë²¡í„° ì €ì¥ì†Œ ìƒì„±")
                # ë”ë¯¸ ë¬¸ì„œë¡œ ì´ˆê¸°í™”
                dummy_doc = Document(
                    page_content="ì´ˆê¸°í™”ìš© ë”ë¯¸ ë¬¸ì„œ",
                    metadata={"type": "dummy", "id": "init"},
                )
                vector_store = FAISS.from_documents([dummy_doc], self.embeddings)
                return vector_store
        except Exception as e:
            logger.error(f"ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ/ìƒì„± ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ë¬¸ì„œë¡œ ìƒˆë¡œ ìƒì„±
            dummy_doc = Document(
                page_content="ì´ˆê¸°í™”ìš© ë”ë¯¸ ë¬¸ì„œ",
                metadata={"type": "dummy", "id": "init"},
            )
            return FAISS.from_documents([dummy_doc], self.embeddings)

    def _load_or_create_metadata(self) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„° íŒŒì¼ ë¡œë“œ ë˜ëŠ” ìƒì„±"""
        try:
            if self.metadata_file.exists():
                with open(self.metadata_file, "r", encoding="utf-8") as f:
                    metadata = json.load(f)
                logger.info(
                    f"ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(metadata.get('posts', {}))} í¬ìŠ¤íŠ¸"
                )
                return metadata
            else:
                # ìƒˆ ë©”íƒ€ë°ì´í„° êµ¬ì¡° ìƒì„±
                metadata = {
                    "created_at": datetime.now().isoformat(),
                    "last_updated": datetime.now().isoformat(),
                    "posts": {},  # post_id -> metadata
                    "keywords": {},  # keyword -> [post_ids]
                    "categories": {},  # category -> [post_ids]
                    "stats": {
                        "total_posts": 0,
                        "total_words": 0,
                        "last_post_date": None,
                    },
                }
                self._save_metadata(metadata)
                logger.info("ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° êµ¬ì¡° ìƒì„±")
                return metadata
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {
                "created_at": datetime.now().isoformat(),
                "last_updated": datetime.now().isoformat(),
                "posts": {},
                "keywords": {},
                "categories": {},
                "stats": {"total_posts": 0, "total_words": 0, "last_post_date": None},
            }

    def _save_metadata(self, metadata: Dict[str, Any] = None):
        """ë©”íƒ€ë°ì´í„°ë¥¼ íŒŒì¼ì— ì €ì¥"""
        if metadata is None:
            metadata = self.metadata

        metadata["last_updated"] = datetime.now().isoformat()

        try:
            with open(self.metadata_file, "w", encoding="utf-8") as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            logger.debug("ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")

    def _save_vector_store(self):
        """ë²¡í„° ì €ì¥ì†Œë¥¼ ë””ìŠ¤í¬ì— ì €ì¥"""
        try:
            self.vector_store.save_local(str(self.vector_store_path))
            logger.debug("ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _generate_content_hash(self, content: str) -> str:
        """ì½˜í…ì¸ ì˜ í•´ì‹œê°’ ìƒì„± (ì¤‘ë³µ ì²´í¬ìš©)"""
        return hashlib.md5(content.encode("utf-8")).hexdigest()

    def store_wordpress_post(
        self,
        post_data: Dict[str, Any],
        content: str,
        keyword: str,
        lsi_keywords: List[str] = None,
        longtail_keywords: List[str] = None,
        categories: List[str] = None,
    ) -> bool:
        """
        ì›Œë“œí”„ë ˆìŠ¤ í¬ìŠ¤íŠ¸ë¥¼ ë²¡í„° ì €ì¥ì†Œì— ì €ì¥

        Args:
            post_data: ì›Œë“œí”„ë ˆìŠ¤ í¬ìŠ¤íŠ¸ ì •ë³´ (id, url, title, date ë“±)
            content: í¬ìŠ¤íŠ¸ ë³¸ë¬¸ ë‚´ìš© (ë§ˆí¬ë‹¤ìš´ ë˜ëŠ” í…ìŠ¤íŠ¸)
            keyword: ì£¼ìš” í‚¤ì›Œë“œ
            lsi_keywords: LSI í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            longtail_keywords: ë¡±í…Œì¼ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            categories: ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            post_id = str(post_data["id"])

            # ì¤‘ë³µ ì²´í¬
            content_hash = self._generate_content_hash(content)
            if post_id in self.metadata["posts"]:
                existing_hash = self.metadata["posts"][post_id].get("content_hash")
                if existing_hash == content_hash:
                    logger.info(f"ë™ì¼í•œ ì½˜í…ì¸ ê°€ ì´ë¯¸ ì €ì¥ë¨: {post_id}")
                    return True

            # ì½˜í…ì¸ ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í•  (ê¸´ ì½˜í…ì¸  ì²˜ë¦¬)
            chunks = self._split_content(content, max_chunk_size=1000)

            # ê° ì²­í¬ë¥¼ ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€
            documents = []
            for i, chunk in enumerate(chunks):
                doc_metadata = {
                    "post_id": post_id,
                    "post_title": post_data.get("title", ""),
                    "post_url": post_data.get("url", ""),
                    "post_date": post_data.get("date", ""),
                    "chunk_index": i,
                    "total_chunks": len(chunks),
                    "keyword": keyword,
                    "lsi_keywords": lsi_keywords or [],
                    "longtail_keywords": longtail_keywords or [],
                    "categories": categories or [],
                    "content_hash": content_hash,
                    "stored_at": datetime.now().isoformat(),
                }

                documents.append(Document(page_content=chunk, metadata=doc_metadata))

            # ë²¡í„° ì €ì¥ì†Œì— ë¬¸ì„œ ì¶”ê°€
            if documents:
                self.vector_store.add_documents(documents)
                logger.info(f"ë²¡í„° ì €ì¥ì†Œì— {len(documents)}ê°œ ì²­í¬ ì¶”ê°€: {post_id}")

            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            self.metadata["posts"][post_id] = {
                "title": post_data.get("title", ""),
                "url": post_data.get("url", ""),
                "date": post_data.get("date", ""),
                "keyword": keyword,
                "lsi_keywords": lsi_keywords or [],
                "longtail_keywords": longtail_keywords or [],
                "categories": categories or [],
                "content_length": len(content),
                "chunk_count": len(chunks),
                "content_hash": content_hash,
                "stored_at": datetime.now().isoformat(),
            }

            # í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
            self._update_keyword_index(
                post_id, keyword, lsi_keywords, longtail_keywords
            )

            # ì¹´í…Œê³ ë¦¬ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
            self._update_category_index(post_id, categories)

            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_stats(content)

            # ì €ì¥
            self._save_metadata()
            self._save_vector_store()

            logger.info(
                f"âœ… í¬ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ: {post_id} ({post_data.get('title', 'No Title')})"
            )
            return True

        except Exception as e:
            logger.error(f"í¬ìŠ¤íŠ¸ ì €ì¥ ì‹¤íŒ¨ ({post_data.get('id', 'Unknown')}): {e}")
            return False

    def _split_content(self, content: str, max_chunk_size: int = 1000) -> List[str]:
        """ì½˜í…ì¸ ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• """
        # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í•  ì‹œë„
        paragraphs = content.split("\n\n")
        chunks = []
        current_chunk = ""

        for paragraph in paragraphs:
            # í˜„ì¬ ì²­í¬ì— ë¬¸ë‹¨ì„ ì¶”ê°€í–ˆì„ ë•Œ í¬ê¸° ì²´í¬
            if len(current_chunk) + len(paragraph) + 2 <= max_chunk_size:
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph
            else:
                # í˜„ì¬ ì²­í¬ë¥¼ ì €ì¥í•˜ê³  ìƒˆ ì²­í¬ ì‹œì‘
                if current_chunk:
                    chunks.append(current_chunk.strip())

                # ë¬¸ë‹¨ì´ ë„ˆë¬´ ê¸´ ê²½ìš° ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
                if len(paragraph) > max_chunk_size:
                    sentences = paragraph.split(". ")
                    temp_chunk = ""
                    for sentence in sentences:
                        if len(temp_chunk) + len(sentence) + 2 <= max_chunk_size:
                            if temp_chunk:
                                temp_chunk += ". " + sentence
                            else:
                                temp_chunk = sentence
                        else:
                            if temp_chunk:
                                chunks.append(temp_chunk.strip())
                            temp_chunk = sentence
                    if temp_chunk:
                        current_chunk = temp_chunk
                else:
                    current_chunk = paragraph

        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks

    def _update_keyword_index(
        self,
        post_id: str,
        keyword: str,
        lsi_keywords: List[str] = None,
        longtail_keywords: List[str] = None,
    ):
        """í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸"""
        all_keywords = [keyword]
        if lsi_keywords:
            all_keywords.extend(lsi_keywords)
        if longtail_keywords:
            all_keywords.extend(longtail_keywords)

        for kw in all_keywords:
            if kw not in self.metadata["keywords"]:
                self.metadata["keywords"][kw] = []
            if post_id not in self.metadata["keywords"][kw]:
                self.metadata["keywords"][kw].append(post_id)

    def _update_category_index(self, post_id: str, categories: List[str] = None):
        """ì¹´í…Œê³ ë¦¬ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸"""
        if not categories:
            return

        for category in categories:
            if category not in self.metadata["categories"]:
                self.metadata["categories"][category] = []
            if post_id not in self.metadata["categories"][category]:
                self.metadata["categories"][category].append(post_id)

    def _update_stats(self, content: str):
        """í†µê³„ ì •ë³´ ì—…ë°ì´íŠ¸"""
        self.metadata["stats"]["total_posts"] += 1
        self.metadata["stats"]["total_words"] += len(content.split())
        self.metadata["stats"]["last_post_date"] = datetime.now().isoformat()

    def find_similar_posts(
        self,
        query_text: str,
        k: int = 5,
        exclude_post_id: str = None,
        min_similarity_score: float = 0.7,
        search_titles_only: bool = False,  # ì œëª©ë§Œ ê²€ìƒ‰ ì˜µì…˜ ì¶”ê°€
    ) -> List[Dict[str, Any]]:
        """
        ìœ ì‚¬í•œ í¬ìŠ¤íŠ¸ ê²€ìƒ‰

        Args:
            query_text: ê²€ìƒ‰í•  í…ìŠ¤íŠ¸
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            exclude_post_id: ì œì™¸í•  í¬ìŠ¤íŠ¸ ID
            min_similarity_score: ìµœì†Œ ìœ ì‚¬ë„ ì ìˆ˜

        Returns:
            ìœ ì‚¬í•œ í¬ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ (ë©”íƒ€ë°ì´í„° í¬í•¨)
        """
        try:
            if not self.vector_store or len(self.metadata["posts"]) == 0:
                return []

            # ì œëª©ë§Œ ê²€ìƒ‰í•˜ëŠ” ê²½ìš°
            if search_titles_only:
                similar_posts = []
                for post_id, post_data in self.metadata["posts"].items():
                    if exclude_post_id and post_id == exclude_post_id:
                        continue

                    title = post_data.get("title", "")
                    if not title:
                        continue

                    # ì œëª©ê³¼ ì¿¼ë¦¬ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
                    query_words = set(query_text.lower().split())
                    title_words = set(title.lower().split())

                    # ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚°
                    intersection = len(query_words & title_words)
                    union = len(query_words | title_words)
                    similarity = intersection / union if union > 0 else 0.0

                    if similarity >= min_similarity_score:
                        similar_posts.append(
                            {
                                "post_id": post_id,
                                "title": title,
                                "url": post_data.get("url", ""),
                                "keyword": post_data.get("keyword", ""),
                                "similarity_score": similarity,
                                "metadata": post_data,
                            }
                        )

                # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
                similar_posts.sort(key=lambda x: x["similarity_score"], reverse=True)
                logger.info(f"ì œëª© ê²€ìƒ‰ ê²°ê³¼: {len(similar_posts)}ê°œ ë°œê²¬")
                return similar_posts[:k]

            # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰ (ì „ì²´ ì½˜í…ì¸ )
            results = self.vector_store.similarity_search_with_score(
                query_text, k=k * 2
            )  # ì—¬ìœ ë¶„ í™•ë³´

            # ê²°ê³¼ ì²˜ë¦¬ ë° í•„í„°ë§
            similar_posts = []
            seen_post_ids = set()

            for doc, score in results:
                post_id = doc.metadata.get("post_id")

                # ì œì™¸í•  í¬ìŠ¤íŠ¸ ID ì²´í¬
                if exclude_post_id and post_id == exclude_post_id:
                    continue

                # ë”ë¯¸ ë¬¸ì„œ ì œì™¸
                if doc.metadata.get("type") == "dummy":
                    continue

                # ì¤‘ë³µ í¬ìŠ¤íŠ¸ ì œì™¸ (ê°™ì€ í¬ìŠ¤íŠ¸ì˜ ë‹¤ë¥¸ ì²­í¬)
                if post_id in seen_post_ids:
                    continue

                # ìœ ì‚¬ë„ ì ìˆ˜ ì²´í¬ (FAISSëŠ” ê±°ë¦¬ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)
                similarity = 1 / (1 + score)  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                if similarity < min_similarity_score:
                    continue

                seen_post_ids.add(post_id)

                # ë©”íƒ€ë°ì´í„°ì—ì„œ í¬ìŠ¤íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                post_metadata = self.metadata["posts"].get(post_id, {})

                similar_posts.append(
                    {
                        "post_id": post_id,
                        "title": post_metadata.get(
                            "title", doc.metadata.get("post_title", "")
                        ),
                        "url": post_metadata.get(
                            "url", doc.metadata.get("post_url", "")
                        ),
                        "keyword": post_metadata.get("keyword", ""),
                        "similarity_score": similarity,
                        "content_preview": (
                            doc.page_content[:200] + "..."
                            if len(doc.page_content) > 200
                            else doc.page_content
                        ),
                    }
                )

                if len(similar_posts) >= k:
                    break

            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            similar_posts.sort(key=lambda x: x["similarity_score"], reverse=True)

            logger.info(
                f"ìœ ì‚¬í•œ í¬ìŠ¤íŠ¸ {len(similar_posts)}ê°œ ë°œê²¬ (ì¿¼ë¦¬: '{query_text[:50]}...')"
            )
            return similar_posts[:k]

        except Exception as e:
            logger.error(f"ìœ ì‚¬í•œ í¬ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def get_posts_by_keyword(self, keyword: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • í‚¤ì›Œë“œë¡œ í¬ìŠ¤íŠ¸ ê²€ìƒ‰"""
        post_ids = self.metadata["keywords"].get(keyword, [])
        return [
            self.metadata["posts"][pid]
            for pid in post_ids
            if pid in self.metadata["posts"]
        ]

    def get_posts_by_category(self, category: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ë¡œ í¬ìŠ¤íŠ¸ ê²€ìƒ‰"""
        post_ids = self.metadata["categories"].get(category, [])
        return [
            self.metadata["posts"][pid]
            for pid in post_ids
            if pid in self.metadata["posts"]
        ]

    def find_posts_by_keyword_similarity(
        self,
        target_keywords: List[str],
        exclude_post_id: str = None,
        min_keyword_match: int = 1,
    ) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ í¬ìŠ¤íŠ¸ ê²€ìƒ‰ (ë©”íƒ€ë°ì´í„° ê¸°ë°˜)

        Args:
            target_keywords: ê²€ìƒ‰í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            exclude_post_id: ì œì™¸í•  í¬ìŠ¤íŠ¸ ID
            min_keyword_match: ìµœì†Œ ë§¤ì¹­ í‚¤ì›Œë“œ ìˆ˜

        Returns:
            ë§¤ì¹­ëœ í¬ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ (ë§¤ì¹­ ì ìˆ˜ í¬í•¨)
        """
        matched_posts = []

        # ëª¨ë“  í‚¤ì›Œë“œë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜
        target_keywords_lower = [kw.lower().strip() for kw in target_keywords]

        for post_id, post_data in self.metadata["posts"].items():
            if exclude_post_id and post_id == exclude_post_id:
                continue

            # í•´ë‹¹ í¬ìŠ¤íŠ¸ì˜ ëª¨ë“  í‚¤ì›Œë“œ ìˆ˜ì§‘
            post_keywords = []
            post_keywords.append(post_data.get("keyword", "").lower())
            post_keywords.extend(
                [kw.lower() for kw in post_data.get("lsi_keywords", [])]
            )
            post_keywords.extend(
                [kw.lower() for kw in post_data.get("longtail_keywords", [])]
            )

            # ë¹ˆ í‚¤ì›Œë“œ ì œê±°
            post_keywords = [kw.strip() for kw in post_keywords if kw.strip()]

            # í‚¤ì›Œë“œ ë§¤ì¹­ ê³„ì‚°
            matches = 0
            matched_keywords = []

            for target_kw in target_keywords_lower:
                for post_kw in post_keywords:
                    # ì™„ì „ ì¼ì¹˜ ë˜ëŠ” ë¶€ë¶„ ì¼ì¹˜ í™•ì¸
                    if target_kw in post_kw or post_kw in target_kw:
                        matches += 1
                        matched_keywords.append((target_kw, post_kw))
                        break  # í•˜ë‚˜ì˜ íƒ€ê²Ÿ í‚¤ì›Œë“œë‹¹ í•˜ë‚˜ì˜ ë§¤ì¹˜ë§Œ

            # ìµœì†Œ ë§¤ì¹­ ì¡°ê±´ í™•ì¸
            if matches >= min_keyword_match:
                # ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (ë§¤ì¹­ëœ í‚¤ì›Œë“œ ìˆ˜ / ì „ì²´ íƒ€ê²Ÿ í‚¤ì›Œë“œ ìˆ˜)
                match_score = matches / len(target_keywords_lower)

                matched_posts.append(
                    {
                        "post_id": post_id,
                        "title": post_data.get("title", ""),
                        "url": post_data.get("url", ""),
                        "keyword": post_data.get("keyword", ""),
                        "match_score": match_score,
                        "matched_count": matches,
                        "matched_keywords": matched_keywords,
                        "metadata": post_data,
                    }
                )

        # ë§¤ì¹­ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        matched_posts.sort(key=lambda x: x["match_score"], reverse=True)

        logger.info(f"í‚¤ì›Œë“œ ë§¤ì¹­ ê²°ê³¼: {len(matched_posts)}ê°œ í¬ìŠ¤íŠ¸ ë°œê²¬")
        return matched_posts

    def get_storage_stats(self) -> Dict[str, Any]:
        """ì €ì¥ì†Œ í†µê³„ ì •ë³´ ë°˜í™˜"""
        return {
            "total_posts": len(self.metadata["posts"]),
            "total_keywords": len(self.metadata["keywords"]),
            "total_categories": len(self.metadata["categories"]),
            "vector_store_size": (
                len(self.vector_store.docstore._dict)
                if hasattr(self.vector_store, "docstore")
                else 0
            ),
            "stats": self.metadata["stats"],
        }


# í¸ì˜ë¥¼ ìœ„í•œ íŒ©í† ë¦¬ í•¨ìˆ˜
def create_content_storage(storage_dir: str = "data/content_storage") -> ContentStorage:
    """ì½˜í…ì¸  ì €ì¥ì†Œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return ContentStorage(storage_dir)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    storage = create_content_storage()
    stats = storage.get_storage_stats()
    print(f"ğŸ“Š ì €ì¥ì†Œ í†µê³„: {stats}")
