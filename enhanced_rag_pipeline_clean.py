#!/usr/bin/env python3
"""
Enhanced RAG Pipeline for SEO Blog Generation - Clean Version
- Detailed cost analysis
- LSI and longtail keywords
- Structured HTML output
- Token tracking
"""

import sys
import os
import time
import json
import asyncio
import base64
import random
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple, Optional
import re
from openai import OpenAI

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.utils.config import load_config
from src.utils.llm_factory import create_gpt5_nano, LLMFactory, LLMConfig
from src.utils.rag import SimpleRAG
from src.utils.image_optimizer import ImageOptimizer
from src.utils.external_link_builder import ExternalLinkBuilder
from langchain_core.messages import HumanMessage


class EnhancedRAGPipeline:
    """Enhanced RAG Pipeline with detailed analytics"""

    def __init__(self):
        self.config = load_config()
        self.llm = create_gpt5_nano()
        self.rag = None
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì´ë¯¸ì§€ ìƒì„±ìš©)
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        # ì´ë¯¸ì§€ ìµœì í™” ë„êµ¬ ì´ˆê¸°í™”
        self.image_optimizer = ImageOptimizer()
        # ì™¸ë¶€ë§í¬ ìƒì„± ë„êµ¬ ì´ˆê¸°í™”
        self.external_link_builder = ExternalLinkBuilder()
        self.cost_tracker = {
            "total_calls": 0,
            "total_tokens": {"prompt": 0, "completion": 0},
            "total_duration": 0,
            "total_images": 0,  # ì´ë¯¸ì§€ ìƒì„± íšŸìˆ˜ ì¶”ì 
            "step_details": [],
            "image_details": [],  # ì´ë¯¸ì§€ ìƒì„± ë¹„ìš© ì¶”ì 
        }

    def _safe_fragment(self, text: str, max_len: int = 120) -> str:
        """ìœˆë„ìš° í˜¸í™˜ íŒŒì¼ëª… ì¡°ê° ìƒì„± (ê¸ˆì§€ë¬¸ì ì œê±°/ì¹˜í™˜)"""
        frag = re.sub(r"[\\/:*?\"<>|]", "_", text)
        frag = re.sub(r"\s+", "_", frag)
        frag = frag.strip("._")
        if not frag:
            frag = "output"
        return frag[:max_len]

    def setup_rag(self, data_dir: str = "data"):
        """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        data_path = project_root / data_dir
        if data_path.exists():
            self.rag = SimpleRAG(docs_dir=str(data_path))
            self.rag.build()
            return self.rag.vs is not None
        return False

    def track_llm_call(
        self,
        step_name: str,
        prompt_tokens: int,
        completion_tokens: int,
        duration: float,
        output: str,
        purpose: str,
    ):
        """LLM í˜¸ì¶œ ì¶”ì """
        self.cost_tracker["total_calls"] += 1
        self.cost_tracker["total_tokens"]["prompt"] += prompt_tokens
        self.cost_tracker["total_tokens"]["completion"] += completion_tokens
        self.cost_tracker["total_duration"] += duration

        # GPT-5-nano ë¹„ìš© ê³„ì‚° (ì¶”ì •)
        input_cost_per_1k = 0.00005  # $0.05 per 1M tokens = $0.00005 per 1K tokens
        output_cost_per_1k = 0.0004  # $0.40 per 1M tokens = $0.0004 per 1K tokens

        step_cost = (prompt_tokens * input_cost_per_1k / 1000) + (
            completion_tokens * output_cost_per_1k / 1000
        )

        self.cost_tracker["step_details"].append(
            {
                "step": step_name,
                "timestamp": datetime.now().strftime("%H:%M:%S"),
                "duration_seconds": duration,
                "tokens": {"prompt": prompt_tokens, "completion": completion_tokens},
                "estimated_cost_usd": step_cost,
                "purpose": purpose,
                "output_summary": output[:100] + "..." if len(output) > 100 else output,
            }
        )

    async def generate_title_keywords(self, keyword: str) -> Dict[str, Any]:
        """ë‹¨ì¼ í˜¸ì¶œ: ì œëª©, LSI í‚¤ì›Œë“œ, ë¡±í…Œì¼ í‚¤ì›Œë“œ ìƒì„±"""
        start_time = time.time()
        prompt = f"""
í‚¤ì›Œë“œ: {keyword}

í•˜ë‚˜ì˜ JSONìœ¼ë¡œ ì•„ë˜ 4ê°€ì§€ë¥¼ ëª¨ë‘ êµ¬ì¡°í™”í•˜ì—¬ ë°˜í™˜í•˜ì„¸ìš”. í•œêµ­ì–´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
1) title: SEO ìµœì í™” ë¸”ë¡œê·¸ ì œëª© (í‚¤ì›Œë“œ í¬í•¨, 60ì ì´ë‚´)
2) lsi_keywords: ì˜ë¯¸ì ìœ¼ë¡œ ì—°ê´€ëœ LSI í‚¤ì›Œë“œ 10ê°œ ë°°ì—´
3) longtail_keywords: êµ¬ì²´ì ì¸ ë¡±í…Œì¼ í‚¤ì›Œë“œ 8ê°œ ë°°ì—´
4) notes: ê°„ë‹¨í•œ ìƒì„± ì˜ë„ ì„¤ëª… 1-2ë¬¸ì¥

ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì˜ ë‹¨ì¼ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”:
{{
  "title": "...",
  "lsi_keywords": ["..."],
  "longtail_keywords": ["..."],
  "notes": "..."
}}
"""
        if self.rag and self.rag.vs:
            rag_context = self.rag.query(f"{keyword} ê´€ë ¨ ë°°ê²½ ì •ë³´", k=2)
            prompt = f"{rag_context}\n\n{prompt}"

        messages = [HumanMessage(content=prompt)]
        response = self.llm.invoke(messages)
        duration = time.time() - start_time
        prompt_tokens = int(len(prompt.split()) * 1.3)
        completion_tokens = int(len(response.content.split()) * 1.3)

        self.track_llm_call(
            "generate_title_keywords",
            prompt_tokens,
            completion_tokens,
            duration,
            response.content,
            "í‚¤ì›Œë“œâ†’ì œëª©/LSI/ë¡±í…Œì¼ ë‹¨ì¼ JSON ìƒì„±",
        )

        try:
            import re, json as _json

            m = re.search(r"\{[\s\S]*\}$", response.content.strip())
            data = _json.loads(m.group(0)) if m else _json.loads(response.content)
            # ë³´ì •
            data.setdefault("lsi_keywords", [])
            data.setdefault("longtail_keywords", [])
            data.setdefault("notes", "")
            return data
        except Exception:
            return {
                "title": f"{keyword} ì™„ë²½ ê°€ì´ë“œ",
                "lsi_keywords": [f"{keyword} íŒ", f"{keyword} ë°©ë²•"],
                "longtail_keywords": [f"{keyword} ì´ˆë³´ ê°€ì´ë“œ"],
                "notes": "ê¸°ë³¸ í´ë°± ê²°ê³¼",
            }

    async def generate_structure_json(
        self, title: str, keyword: str, lsi: List[str], longtail: List[str]
    ) -> Dict[str, Any]:
        """ë¸”ë¡œê·¸ êµ¬ì¡° JSON ìƒì„± (7-10ê°œ H2 ì„¹ì…˜, ê°œìš”+ë§ˆë¬´ë¦¬+FAQ í¬í•¨)"""
        import random

        start_time = time.time()
        target_sections = random.randint(7, 10)
        joined = ", ".join((lsi or [])[:5] + (longtail or [])[:3])
        prompt = f"""
ì œëª©: {title}
í‚¤ì›Œë“œ: {keyword}
ì—°ê´€ í‚¤ì›Œë“œ: {joined}

ì•„ë˜ ì¡°ê±´ìœ¼ë¡œ ë¸”ë¡œê·¸ ë¬¸ì„œ êµ¬ì¡°ë¥¼ JSONìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”.
- H2 ì„¹ì…˜ ìˆ˜: ì •í™•íˆ {target_sections}ê°œ
- ì²« ë²ˆì§¸ H2ëŠ” ë°˜ë“œì‹œ 'ê°œìš”', 'ì†Œê°œ', 'ì‹œì‘í•˜ê¸°' ì¤‘ í•˜ë‚˜ì˜ ì„±ê²©ì„ ê°€ì§„ ë„ì… ì„¹ì…˜ì´ì–´ì•¼ í•©ë‹ˆë‹¤ (ììœ ë¡­ê²Œ í‘œí˜„ ê°€ëŠ¥)
- ë§ˆì§€ë§‰ì—ì„œ ë‘ ë²ˆì§¸ H2ëŠ” 'ì •ë¦¬ì™€ ë§ˆë¬´ë¦¬', 'ìš”ì•½ê³¼ ê²°ë¡ ', 'í•µì‹¬ í¬ì¸íŠ¸' ë“± ì „ì²´ ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” ì„¹ì…˜ì´ì–´ì•¼ í•©ë‹ˆë‹¤
- ë§ˆì§€ë§‰ H2ëŠ” ë°˜ë“œì‹œ 'ìì£¼ ë¬»ëŠ” ì§ˆë¬¸', 'FAQ', 'ê¶ê¸ˆí•œ ì ë“¤' ë“± ì§ˆë¬¸-ë‹µë³€ í˜•íƒœì˜ ì„¹ì…˜ì´ì–´ì•¼ í•©ë‹ˆë‹¤
- ê° H2ë§ˆë‹¤ H3/H4ëŠ” ìœ ë™ì ìœ¼ë¡œ 0ê°œ ì´ìƒ í¬í•¨ ê°€ëŠ¥
- í•œêµ­ì–´ ì œëª© ì‚¬ìš©, í‚¤ì›Œë“œëŠ” ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨

ë°˜í™˜ í˜•ì‹ ì˜ˆì‹œ:
{{
  "title": "{title}",
  "sections": [
    {{
      "h2": "ì„¹ì…˜ ì œëª©",
      "h3": ["ì†Œì œëª©1", "ì†Œì œëª©2"],
      "h4_map": {{"ì†Œì œëª©1": ["í•­ëª©1", "í•­ëª©2"]}}
    }}
  ]
}}
ë°˜ë“œì‹œ ìœ„ì˜ JSON ìŠ¤í‚¤ë§ˆë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""
        if self.rag and self.rag.vs:
            rag_context = self.rag.query(f"{keyword} ì „ì²´ êµ¬ì¡° ì„¤ê³„ ì°¸ê³ ", k=3)
            prompt = f"{rag_context}\n\n{prompt}"

        messages = [HumanMessage(content=prompt)]
        response = self.llm.invoke(messages)
        duration = time.time() - start_time
        prompt_tokens = int(len(prompt.split()) * 1.3)
        completion_tokens = int(len(response.content.split()) * 1.3)

        self.track_llm_call(
            "structure_generation",
            prompt_tokens,
            completion_tokens,
            duration,
            response.content,
            "7-10ê°œì˜ H2ì™€ ìœ ë™ H3/H4 êµ¬ì¡° JSON (ê°œìš”+ë§ˆë¬´ë¦¬+FAQ í¬í•¨)",
        )

        try:
            import re, json as _json

            m = re.search(r"\{[\s\S]*\}$", response.content.strip())
            data = _json.loads(m.group(0)) if m else _json.loads(response.content)
            return data
        except Exception:
            return {"title": title, "sections": []}

    async def summarize_previous(self, text: str) -> str:
        """ì´ì „ ì„¹ì…˜ ë‚´ìš© ìš”ì•½"""
        prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ë§Œ 2-3ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ ìš”ì•½:
---
{text}
---
ìš”ì•½:
"""
        messages = [HumanMessage(content=prompt)]
        response = self.llm.invoke(messages)
        return response.content.strip()

    async def generate_image(self, prompt: str, purpose: str) -> Optional[str]:
        """ì´ë¯¸ì§€ ìƒì„± (gpt-image-1 ëª¨ë¸ ì‚¬ìš©)

        Args:
            prompt: ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ ì˜ë¬¸ í”„ë¡¬í”„íŠ¸
            purpose: ì´ë¯¸ì§€ ìš©ë„ (cost trackingìš©)

        Returns:
            ìƒì„±ëœ ì´ë¯¸ì§€ì˜ base64 ë¬¸ìì—´ ë˜ëŠ” None
        """
        try:
            start_time = time.time()

            # OpenAI Image API í˜¸ì¶œ (gpt-image-1ì€ í•­ìƒ base64ë¡œ ë°˜í™˜)
            response = self.openai_client.images.generate(
                model="gpt-image-1",
                prompt=prompt,
                quality="low",  # ì €í’ˆì§ˆ (ê°€ê²© íš¨ìœ¨ì„±)
                size="1024x1024",  # í‘œì¤€ ì‚¬ì´ì¦ˆ
                n=1,  # 1ê°œ ì´ë¯¸ì§€
            )

            duration = time.time() - start_time

            # ë¹„ìš© ê³„ì‚° (gpt-image-1 low quality 1024x1024: $0.011)
            image_cost = 0.011

            # ì´ë¯¸ì§€ ìƒì„± ì¶”ì 
            self.cost_tracker["total_images"] += 1
            self.cost_tracker["image_details"].append(
                {
                    "purpose": purpose,
                    "timestamp": datetime.now().strftime("%H:%M:%S"),
                    "duration_seconds": duration,
                    "prompt": prompt[:100] + "..." if len(prompt) > 100 else prompt,
                    "cost_usd": image_cost,
                    "model": "gpt-image-1",
                    "quality": "low",
                    "size": "1024x1024",
                }
            )

            # base64 ì´ë¯¸ì§€ ë°ì´í„° ë°˜í™˜ (gpt-image-1ì€ í•­ìƒ b64_json í˜•íƒœ)
            return response.data[0].b64_json

        except Exception as e:
            print(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨ ({purpose}): {e}")
            return None

    def save_image_from_base64(
        self, b64_data: str, file_path: Path, optimize: bool = True
    ) -> bool:
        """base64 ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥ ë° ìµœì í™”

        Args:
            b64_data: base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë°ì´í„°
            file_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
            optimize: ì´ë¯¸ì§€ ìµœì í™” ì—¬ë¶€

        Returns:
            ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # base64 ë””ì½”ë”©í•˜ì—¬ PNG íŒŒì¼ë¡œ ì €ì¥
            image_data = base64.b64decode(b64_data)
            file_path.parent.mkdir(exist_ok=True)

            with open(file_path, "wb") as f:
                f.write(image_data)

            # ì´ë¯¸ì§€ ìµœì í™” (ì˜µì…˜)
            if optimize:
                # ë¸”ë¡œê·¸ìš© ìµœì í™”: 512x512 ì´í•˜, 50KB ì´í•˜ë¡œ ì••ì¶•
                optimization_result = self.image_optimizer.optimize_for_web(
                    file_path,
                    max_size=(512, 512),
                    target_file_size_kb=50,
                    quality_range=(70, 90),
                )

                if optimization_result["success"]:
                    reduction = optimization_result["size_reduction_percent"]
                    print(
                        f"     ğŸ“‰ ì´ë¯¸ì§€ ìµœì í™”: {optimization_result['file_size_change']} ({reduction}% ê°ì†Œ)"
                    )
                else:
                    print(
                        f"     âš ï¸ ì´ë¯¸ì§€ ìµœì í™” ì‹¤íŒ¨: {optimization_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
                    )

            return True
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    async def generate_section_with_context(
        self,
        idx: int,
        total: int,
        section: Dict[str, Any],
        keyword: str,
        title: str,
        full_structure_json: Dict[str, Any],
        prev_summary: str = "",
        next_h2: str = "",
    ) -> str:
        """ì„¹ì…˜ë³„ ì½˜í…ì¸  ìƒì„± (ì»¨í…ìŠ¤íŠ¸ì™€ í‹°ì € í¬í•¨)"""
        start_time = time.time()
        structure_str = json.dumps(full_structure_json, ensure_ascii=False)
        ctx = f"ì´ì „ ì„¹ì…˜ ìš”ì•½: {prev_summary}\n" if prev_summary else ""

        # í‹°ì € ë¬¸ì¥ ê°€ì´ë“œ (ëª…ì‹œì  í‘œí˜„ ê¸ˆì§€)
        teaser = (
            f"ë§ˆì§€ë§‰ ë¬¸ì¥ì€ '{next_h2}'ì™€ ì£¼ì œê°€ ë§ë‹¿ì•„ ìˆìŒì„ ë…ìê°€ ì•”ì‹œì ìœ¼ë¡œ ëŠë¼ë„ë¡, êµ¬ì²´ì  ì •ë³´ í•œ ì¡°ê°ìœ¼ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”. \n- ê¸ˆì§€ í‘œí˜„: 'ë‹¤ìŒ', 'ë‹¤ìŒ ì„¹ì…˜', 'ë‹¤ìŒ ì±•í„°', 'ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§‘ë‹ˆë‹¤'"
            if next_h2
            else ""
        )

        # ê¸¸ì´ ì •ì±…: 1ì„¹ì…˜ 300ì ë‚´ì™¸, ê·¸ ì™¸ 500-800ì
        length_rule = "ë¶„ëŸ‰: ì•½ 300ì" if idx == 1 else "ë¶„ëŸ‰: 500-800ì"

        prompt = f"""
ë¬¸ì„œ ì œëª©: {title}
ì£¼ìš” í‚¤ì›Œë“œ: {keyword}
ì „ì²´ ë¬¸ì„œ êµ¬ì¡°(JSON): {structure_str}
í˜„ì¬ ì„¹ì…˜: {idx}/{total} - H2: {section.get('h2')}
{ctx}
ìš”êµ¬ì‚¬í•­:
1) í•œêµ­ì–´ ìì—°ìŠ¤ëŸ¬ìš´ ë³¸ë¬¸
2) H3 ì†Œì œëª© 2-3ê°œ í¬í•¨ ê°€ëŠ¥ (ìˆë‹¤ë©´ Markdown ### ì‚¬ìš©)
3) ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ì¸ ë‚´ìš©
4) {length_rule}
5) {teaser}
6) ì¶œë ¥ì€ "ë³¸ë¬¸ë§Œ" ì‘ì„±. ì„¹ì…˜ ì œëª©ì´ë‚˜ H2(##)ë¥¼ ì¶œë ¥í•˜ì§€ ë§ ê²ƒ. 'ì„¹ì…˜ ë³¸ë¬¸:' ê°™ì€ ì•ˆë‚´ ë¬¸êµ¬ë„ ê¸ˆì§€.
7) ì²« ì¤„ì— ì„¹ì…˜ ì œëª©ì„ ë°˜ë³µí•˜ì§€ ë§ ê²ƒ. í•„ìš” ì‹œ H3(###)ë¶€í„° ì‹œì‘.
8) í‘œ í˜•íƒœë¡œ í‘œí˜„í•˜ë©´ ë” íš¨ê³¼ì ì¸ ë‚´ìš©ì´ ìˆë‹¤ë©´ Markdown í‘œ ë¬¸ë²• ì‚¬ìš©:
   - ë¹„êµí‘œ: | í•­ëª© | ì„¤ëª… | ë¹„ê³  |
   - ë‹¨ê³„ë³„ í‘œ: | ë‹¨ê³„ | ë‚´ìš© | íŒ |
   - íŠ¹ì§•í‘œ: | íŠ¹ì§• | ì¥ì  | ë‹¨ì  |
   - ì˜ˆì‹œ: | êµ¬ë¶„ | ë°©ë²• | íš¨ê³¼ |
   - ë„êµ¬ ë¹„êµ: | ë„êµ¬ëª… | ì¥ì  | ë‹¨ì  | ê°€ê²© |
   - ë‹¨ê³„ë³„ ê°€ì´ë“œ: | ë‹¨ê³„ | ì„¤ëª… | ì£¼ì˜ì‚¬í•­ |
   - íŒ ì •ë¦¬: | ìƒí™© | í•´ê²°ë°©ë²• | íš¨ê³¼ |

ë³¸ë¬¸ ì¶œë ¥ ì‹œì‘:
"""
        if self.rag and self.rag.vs:
            rag_context = self.rag.query(f"{keyword} {section.get('h2','')} ë°°ê²½", k=2)
            prompt = f"{rag_context}\n\n{prompt}"

        messages = [HumanMessage(content=prompt)]
        response = self.llm.invoke(messages)
        duration = time.time() - start_time
        prompt_tokens = int(len(prompt.split()) * 1.3)
        completion_tokens = int(len(response.content.split()) * 1.3)

        self.track_llm_call(
            f"section_{idx}",
            prompt_tokens,
            completion_tokens,
            duration,
            response.content,
            f"ì„¹ì…˜ {idx} ë³¸ë¬¸ ìƒì„±",
        )

        return response.content.strip()

    def _sanitize_section_content(self, h2_title: str, content: str) -> str:
        """ëª¨ë¸ ì‘ë‹µì—ì„œ ì¤‘ë³µ H2/ì•ˆë‚´ë¬¸ ë“±ì„ ì œê±°í•˜ì—¬ ê¹”ë”í•œ ë³¸ë¬¸ë§Œ ë‚¨ê¸´ë‹¤."""
        lines = [ln.rstrip() for ln in content.strip().split("\n")]
        sanitized: list[str] = []
        skip_prefixes = {
            "ì„¹ì…˜ ë³¸ë¬¸:",
            "ë³¸ë¬¸:",
            "ë³¸ë¬¸ ì¶œë ¥:",
            "ë³¸ë¬¸ ì¶œë ¥ ì‹œì‘:",
            "ë‚´ìš©:",
        }

        for i, ln in enumerate(lines):
            # ì²« ë¶€ë¶„ì—ì„œë§Œ ê°•ì œ ì œê±° ê·œì¹™ ì ìš©
            if i < 5:
                # 'ì„¹ì…˜ ë³¸ë¬¸:' ë¥˜ ì•ˆë‚´ë¬¸ ì œê±°
                if any(ln.strip().startswith(p) for p in skip_prefixes):
                    continue
                # ì¤‘ë³µ H2 ì œê±° (## ...)
                if ln.strip().startswith("## "):
                    continue
                # ì„¹ì…˜ ì œëª© ë°˜ë³µ í…ìŠ¤íŠ¸ ì œê±° (ì œëª©ë§Œ ë‹¨ë… ë¼ì¸)
                if ln.strip() == h2_title.strip():
                    continue
            sanitized.append(ln)

        # ì•ë’¤ ê³µë°± ì •ë¦¬ ë° ì—°ì† ë¹ˆ ì¤„ ì¶•ì†Œ
        out = "\n".join(sanitized).strip()
        out = re.sub(r"\n\s*\n\s*\n+", "\n\n", out)
        return out

    def create_markdown(
        self,
        title: str,
        keyword: str,
        sections_content: List[Dict[str, Any]],
        keywords: Dict[str, List[str]],
        images: Optional[Dict[str, str]] = None,
    ) -> str:
        """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ìƒì„± (ì´ë¯¸ì§€ í¬í•¨)"""
        md_content = f"""# {title}

**íƒ€ê²Ÿ í‚¤ì›Œë“œ:** {keyword}
**ì˜ˆìƒ ê¸¸ì´:** {sum(len(section['content']) for section in sections_content):,}ì
**SEO ì „ëµ:** í•µì‹¬ í‚¤ì›Œë“œì™€ LSI í‚¤ì›Œë“œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì œëª© ë° ë³¸ë¬¸ì— ë°°ì¹˜í•˜ì—¬ SEO íš¨ê³¼ ê·¹ëŒ€í™”

**LSI í‚¤ì›Œë“œ:** {', '.join(keywords['lsi_keywords'])}
**ë¡±í…Œì¼ í‚¤ì›Œë“œ:** {', '.join(keywords['longtail_keywords'])}

"""

        # ë©”ì¸ ì´ë¯¸ì§€ ì¶”ê°€ (ì œëª© ê¸°ë°˜) - ì›Œë“œí”„ë ˆìŠ¤ í˜¸í™˜ ìŠ¤íƒ€ì¼
        if images and "main" in images:
            md_content += f'![{title}]({images["main"]})\n\n'

        for i, section in enumerate(sections_content):
            md_content += f"## {section['h2_title']}\n\n"

            # ì„¹ì…˜ ì´ë¯¸ì§€ ì¶”ê°€ (33% í™•ë¥ ë¡œ) - ì›Œë“œí”„ë ˆìŠ¤ í˜¸í™˜ ìŠ¤íƒ€ì¼
            section_image_key = f"section_{i+1}"
            if images and section_image_key in images:
                md_content += (
                    f'![{section["h2_title"]}]({images[section_image_key]})\n\n'
                )

            md_content += f"{section['content']}\n\n"

        return md_content

    def create_cost_analysis_report(
        self,
        keyword: str,
        title: str,
        keywords: Dict,
        sections_content: List,
        total_duration: float,
    ) -> Dict[str, Any]:
        """ìƒì„¸í•œ ë¹„ìš© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        # í…ìŠ¤íŠ¸ ìƒì„± ë¹„ìš©
        text_cost = sum(
            step["estimated_cost_usd"] for step in self.cost_tracker["step_details"]
        )
        # ì´ë¯¸ì§€ ìƒì„± ë¹„ìš©
        image_cost = sum(img["cost_usd"] for img in self.cost_tracker["image_details"])
        total_cost = text_cost + image_cost

        total_tokens = (
            self.cost_tracker["total_tokens"]["prompt"]
            + self.cost_tracker["total_tokens"]["completion"]
        )

        report = {
            "report_info": {
                "keyword": keyword,
                "title": title,
                "generation_date": datetime.now().isoformat(),
                "report_version": "2.0 - Enhanced RAG Pipeline",
            },
            "cost_analysis": {
                "pipeline_summary": {
                    "model_used": "gpt-5-nano + gpt-image-1",
                    "total_duration_seconds": round(total_duration, 1),
                    "sections_generated": len(sections_content),
                    "images_generated": self.cost_tracker["total_images"],
                    "total_estimated_cost_usd": round(total_cost, 6),
                    "text_cost_usd": round(text_cost, 6),
                    "image_cost_usd": round(image_cost, 6),
                    "cost_per_section": (
                        round(total_cost / len(sections_content), 6)
                        if sections_content
                        else 0
                    ),
                    "estimated_total_tokens": total_tokens,
                    "status": "completed",
                },
                "llm_call_breakdown": {},
                "step_by_step_analysis": {},
                "cost_optimization_analysis": {
                    "current_efficiency": {
                        "model": "gpt-5-nano",
                        "input_cost_per_1m_tokens": "$0.05",
                        "output_cost_per_1m_tokens": "$0.40",
                        "average_cost_per_section": (
                            f"${round(total_cost / len(sections_content), 6)}"
                            if sections_content
                            else "$0"
                        ),
                        "total_sections_possible_with_1_dollar": (
                            int(1 / (total_cost / len(sections_content)))
                            if sections_content and total_cost > 0
                            else 0
                        ),
                    }
                },
            },
            "keyword_analysis": {
                "primary_keyword": keyword,
                "lsi_keywords": keywords.get("lsi_keywords", []),
                "longtail_keywords": keywords.get("longtail_keywords", []),
                "keyword_density": "ìì—°ìŠ¤ëŸ½ê²Œ ë°°ì¹˜ë¨",
            },
            "content_analysis": {"sections": []},
            "model_configuration": {
                "primary_model": "gpt-5-nano",
                "temperature": 1.0,
                "max_tokens": "unlimited",
                "provider": "OpenAI",
                "rag_enabled": self.rag is not None and self.rag.vs is not None,
            },
            "performance_metrics": {
                "tokens_per_second": (
                    round(total_tokens / total_duration, 1) if total_duration > 0 else 0
                ),
                "cost_per_minute": (
                    f"${round(total_cost * 60 / total_duration, 6)}"
                    if total_duration > 0
                    else "$0"
                ),
                "characters_generated": sum(
                    len(section["content"]) for section in sections_content
                ),
                "cost_per_character": (
                    f"${round(total_cost / sum(len(section['content']) for section in sections_content), 8)}"
                    if sections_content
                    else "$0"
                ),
            },
        }

        # ë‹¨ê³„ë³„ ë¶„ì„ ì¶”ê°€ (í…ìŠ¤íŠ¸)
        for i, step in enumerate(self.cost_tracker["step_details"], 1):
            report["cost_analysis"]["step_by_step_analysis"][
                f"step_{i}_{step['step']}"
            ] = {
                "timestamp": step["timestamp"],
                "duration": f"{step['duration_seconds']:.1f}ì´ˆ",
                "model_calls": 1,
                "cost": f"${step['estimated_cost_usd']:.6f}",
                "output": step["output_summary"],
                "type": "text_generation",
            }

        # ì´ë¯¸ì§€ ìƒì„± ë¶„ì„ ì¶”ê°€
        for i, img in enumerate(self.cost_tracker["image_details"], 1):
            report["cost_analysis"]["step_by_step_analysis"][
                f"image_{i}_{img['purpose']}"
            ] = {
                "timestamp": img["timestamp"],
                "duration": f"{img['duration_seconds']:.1f}ì´ˆ",
                "model_calls": 1,
                "cost": f"${img['cost_usd']:.3f}",
                "output": img["prompt"],
                "type": "image_generation",
                "model": img["model"],
                "quality": img["quality"],
                "size": img["size"],
            }

        # ì„¹ì…˜ë³„ ë¶„ì„ ì¶”ê°€
        for i, section in enumerate(sections_content):
            report["content_analysis"]["sections"].append(
                {
                    "section_number": i + 1,
                    "h2_title": section["h2_title"],
                    "character_count": len(section["content"]),
                    "estimated_tokens": len(section["content"].split()) * 1.3,
                    "h3_count": section["content"].count("### "),
                    "keywords_used": section.get("target_keywords", []),
                }
            )

        return report

    async def run_complete_pipeline(self, keyword: str) -> Dict[str, Any]:
        """ì™„ì „í•œ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = time.time()

        print(f"í‚¤ì›Œë“œ '{keyword}'ë¡œ Enhanced RAG íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("=" * 60)

        try:
            # 1. RAG ì„¤ì •
            rag_enabled = self.setup_rag()
            print(f"1. RAG ì‹œìŠ¤í…œ: {'í™œì„±í™”' if rag_enabled else 'ë¹„í™œì„±í™”'}")

            # 2. ë‹¨ì¼ í˜¸ì¶œ: ì œëª©+í‚¤ì›Œë“œ JSON
            print("2. ì œëª©/í‚¤ì›Œë“œ ë‹¨ì¼ JSON ìƒì„± ì¤‘...")
            tk = await self.generate_title_keywords(keyword)
            print(f"   ì œëª©: {tk['title']}")

            # ì €ì¥ (STEP1)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_kw = self._safe_fragment(keyword)
            step1_file = (
                project_root / f"data/title_keywords_{safe_kw}_{timestamp}.json"
            )
            with open(step1_file, "w", encoding="utf-8") as f:
                json.dump(tk, f, ensure_ascii=False, indent=2)
            alias_step1 = project_root / f"data/OUTPUT_{safe_kw}_step1.json"
            with open(alias_step1, "w", encoding="utf-8") as f:
                json.dump(tk, f, ensure_ascii=False, indent=2)

            # 3. êµ¬ì¡° JSON (7-10ê°œ H2)
            print("3. êµ¬ì¡°(JSON) ìƒì„± ì¤‘...")
            structure = await self.generate_structure_json(
                tk["title"],
                keyword,
                tk.get("lsi_keywords", []),
                tk.get("longtail_keywords", []),
            )
            step2_file = project_root / f"data/structure_{safe_kw}_{timestamp}.json"
            with open(step2_file, "w", encoding="utf-8") as f:
                json.dump(structure, f, ensure_ascii=False, indent=2)
            alias_step2 = project_root / f"data/OUTPUT_{safe_kw}_structure.json"
            with open(alias_step2, "w", encoding="utf-8") as f:
                json.dump(structure, f, ensure_ascii=False, indent=2)

            sections = structure.get("sections", [])
            print(f"   ì„¹ì…˜ ìˆ˜: {len(sections)}ê°œ")

            # 4. ì„¹ì…˜ë³„ ë³¸ë¬¸ ìƒì„± (ì—°ì‡„ ìš”ì•½ ì‚¬ìš©)
            print("4. ì„¹ì…˜ë³„ ì½˜í…ì¸  ìƒì„± ì¤‘...")
            sections_content = []
            prev_summary = ""
            total = len(sections)

            for i, sec in enumerate(sections, 1):
                next_h2 = sections[i]["h2"] if i < total else ""
                raw = await self.generate_section_with_context(
                    idx=i,
                    total=total,
                    section=sec,
                    keyword=keyword,
                    title=tk["title"],
                    full_structure_json=structure,
                    prev_summary=prev_summary,
                    next_h2=next_h2,
                )

                # ëª¨ë¸ ì‘ë‹µ í›„ ì •ë¦¬: ì¤‘ë³µ H2/ì•ˆë‚´ë¬¸ ì œê±°
                content = self._sanitize_section_content(sec.get("h2", ""), raw)
                sections_content.append(
                    {
                        "h2_title": sec.get("h2", f"ì„¹ì…˜ {i}"),
                        "content": content,
                        "target_keywords": [],
                    }
                )

                # ë‹¤ìŒì„ ìœ„í•œ ìš”ì•½ ìƒì„±
                prev_summary = await self.summarize_previous(content)

            # 5. ì™¸ë¶€ë§í¬ ìƒì„± (ì´ˆê¸° ì½˜í…ì¸ ìš©)
            print("5. ì™¸ë¶€ë§í¬ ìƒì„± ì¤‘...")
            content_count = 1  # TODO: ì‹¤ì œ ì½˜í…ì¸  ìˆ˜ ì¶”ì  ì‹œìŠ¤í…œ êµ¬í˜„ ì˜ˆì •

            # ì„ì‹œ ë§ˆí¬ë‹¤ìš´ ìƒì„± (í‚¤ì›Œë“œ ì¶”ì¶œìš©)
            temp_md_content = self.create_markdown(
                tk["title"],
                keyword,
                sections_content,
                {
                    "lsi_keywords": tk.get("lsi_keywords", []),
                    "longtail_keywords": tk.get("longtail_keywords", []),
                },
                {},  # ì´ë¯¸ì§€ëŠ” ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ
            )

            external_links = self.external_link_builder.generate_external_links(
                keywords_data={
                    "lsi_keywords": tk.get("lsi_keywords", []),
                    "longtail_keywords": tk.get("longtail_keywords", []),
                },
                target_keyword=keyword,
                content_count=content_count,
                markdown_content=temp_md_content,  # ì‹¤ì œ ë³¸ë¬¸ ì „ë‹¬
            )

            link_summary = self.external_link_builder.get_links_summary(external_links)
            print(f"   ğŸ“ ìƒì„±ëœ ë§í¬: {link_summary['ì´_ë§í¬_ìˆ˜']}ê°œ")
            print(f"     - ì™¸ë¶€ë§í¬: {link_summary['ì™¸ë¶€ë§í¬_ìˆ˜']}ê°œ")
            print(f"     - í™ˆí˜ì´ì§€: {link_summary['í™ˆí˜ì´ì§€_ë§í¬_ìˆ˜']}ê°œ")

            # 6. ì´ë¯¸ì§€ ìƒì„± (ì œëª© + ì„¹ì…˜ë³„ 20% í™•ë¥ )
            print("6. ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
            images = {}

            # ë©”ì¸ ì´ë¯¸ì§€ (ì œëª© ê¸°ë°˜, 100% í™•ë¥ )
            print("   - ë©”ì¸ ì´ë¯¸ì§€ ìƒì„±...")
            main_prompt = f"Clean minimalist infographic diagram about '{keyword}', visual concept illustration, no text or letters, chart elements, flow diagram style, professional design, simple color scheme"
            main_image_b64 = await self.generate_image(main_prompt, "main_title")

            if main_image_b64:
                main_image_filename = f"main_{safe_kw}.png"
                main_image_path = project_root / f"data/images/{main_image_filename}"
                if self.save_image_from_base64(main_image_b64, main_image_path):
                    # ì›Œë“œí”„ë ˆìŠ¤ í˜¸í™˜ URL í˜•íƒœ (í–¥í›„ ì—…ë¡œë“œ ì‹œ êµì²´ ì˜ˆì •)
                    images["main"] = (
                        f"https://your-wordpress-site.com/wp-content/uploads/2024/images/{main_image_filename}"
                    )
                    print(f"     âœ… ë©”ì¸ ì´ë¯¸ì§€ ì €ì¥: {main_image_filename}")

            # ì„¹ì…˜ë³„ ì´ë¯¸ì§€ (20% í™•ë¥ )
            for i, section in enumerate(sections_content):
                if random.random() < 0.2:  # 20% í™•ë¥ 
                    print(f"   - ì„¹ì…˜ {i+1} ì´ë¯¸ì§€ ìƒì„±: {section['h2_title']}")
                    section_prompt = f"Simple visual diagram for '{section['h2_title']}' concept related to {keyword}, no text or words, minimalist chart design, geometric shapes, clean infographic elements, conceptual illustration"
                    section_image_b64 = await self.generate_image(
                        section_prompt, f"section_{i+1}"
                    )

                    if section_image_b64:
                        section_image_filename = f"section_{i+1}_{safe_kw}.png"
                        section_image_path = (
                            project_root / f"data/images/{section_image_filename}"
                        )
                        if self.save_image_from_base64(
                            section_image_b64, section_image_path
                        ):
                            # ì›Œë“œí”„ë ˆìŠ¤ í˜¸í™˜ URL í˜•íƒœ (í–¥í›„ ì—…ë¡œë“œ ì‹œ êµì²´ ì˜ˆì •)
                            images[f"section_{i+1}"] = (
                                f"https://your-wordpress-site.com/wp-content/uploads/2024/images/{section_image_filename}"
                            )
                            print(f"     âœ… ì„¹ì…˜ ì´ë¯¸ì§€ ì €ì¥: {section_image_filename}")

            total_duration = time.time() - start_time

            # 7. íŒŒì¼ ìƒì„± (ë³„ì¹­ í¬í•¨)
            print("7. íŒŒì¼ ìƒì„± ì¤‘...")
            timestamp2 = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_kw2 = self._safe_fragment(keyword)

            # MD (ì´ë¯¸ì§€ + ì™¸ë¶€ë§í¬ í¬í•¨)
            md_content = self.create_markdown(
                tk["title"],
                keyword,
                sections_content,
                {
                    "lsi_keywords": tk.get("lsi_keywords", []),
                    "longtail_keywords": tk.get("longtail_keywords", []),
                },
                images,  # ì´ë¯¸ì§€ ì •ë³´ ì „ë‹¬
            )

            # ì™¸ë¶€ë§í¬ ì‚½ì… ì „ì— ì›ë³¸ ë§í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°±ì—…
            original_external_links = external_links.copy()

            # ì™¸ë¶€ë§í¬ ì‚½ì… (ë§í¬ ë¦¬ìŠ¤íŠ¸ê°€ ìˆ˜ì •ë  ìˆ˜ ìˆìŒ)
            md_content = self.external_link_builder.insert_links_into_markdown(
                md_content, external_links
            )

            # ì‹¤ì œ ì ìš©ëœ ë§í¬ ìˆ˜ ì¬ê³„ì‚° (ì›ë³¸ ë§í¬ ë¦¬ìŠ¤íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ ì—ì„œ í™•ì¸)
            applied_links = []
            unused_links = []

            for link in original_external_links:
                # ë§ˆí¬ë‹¤ìš´ ë§í¬ íŒ¨í„´ì´ ì¡´ì¬í•˜ëŠ”ì§€ ì²´í¬: [ì•µì»¤í…ìŠ¤íŠ¸](ì–´ë–¤URLì´ë“ )
                link_pattern = f"[{link.anchor_text}]("
                if link_pattern in md_content:
                    applied_links.append(link)
                else:
                    unused_links.append(link)
            md_file = project_root / f"data/blog_{safe_kw2}_{timestamp2}.md"
            md_file.parent.mkdir(exist_ok=True)
            with open(md_file, "w", encoding="utf-8") as f:
                f.write(md_content)
            alias_md = project_root / f"data/OUTPUT_{safe_kw2}.md"
            with open(alias_md, "w", encoding="utf-8") as f:
                f.write(md_content)

            # HTML (SimpleHTMLConverter ì‚¬ìš©)
            from src.generators.html.simple_html_converter import SimpleHTMLConverter

            converter = SimpleHTMLConverter()
            html_content = converter.convert_markdown_to_html(md_content)
            html_file = project_root / f"data/blog_{safe_kw2}_{timestamp2}_final.html"
            with open(html_file, "w", encoding="utf-8") as f:
                f.write(html_content)
            alias_html = project_root / f"data/OUTPUT_{safe_kw2}.html"
            with open(alias_html, "w", encoding="utf-8") as f:
                f.write(html_content)

            # ë¹„ìš© ë¦¬í¬íŠ¸ (ì™¸ë¶€ë§í¬ ì •ë³´ í¬í•¨)
            cost_report = self.create_cost_analysis_report(
                keyword,
                tk["title"],
                {
                    "lsi_keywords": tk.get("lsi_keywords", []),
                    "longtail_keywords": tk.get("longtail_keywords", []),
                },
                sections_content,
                total_duration,
            )

            # ì™¸ë¶€ë§í¬ ì •ë³´ ì¶”ê°€ (ì‹¤ì œ ì ìš©ëœ ë§í¬ ê¸°ì¤€)
            cost_report["link_analysis"] = {
                "external_links_generated": len(applied_links),
                "link_summary": {
                    "ì´_ë§í¬_ìˆ˜": len(applied_links),
                    "ì™¸ë¶€ë§í¬_ìˆ˜": len(
                        [l for l in applied_links if l.platform != "í™ˆí˜ì´ì§€"]
                    ),
                    "í™ˆí˜ì´ì§€_ë§í¬_ìˆ˜": len(
                        [l for l in applied_links if l.platform == "í™ˆí˜ì´ì§€"]
                    ),
                    "í”Œë«í¼ë³„": {},
                },
                "link_details": [
                    {
                        "anchor_text": link.anchor_text,
                        "url": link.url,
                        "platform": link.platform,
                        "keyword_type": link.keyword_type,
                    }
                    for link in applied_links
                ],
                "unused_links": [
                    {
                        "anchor_text": link.anchor_text,
                        "url": link.url,
                        "platform": link.platform,
                        "keyword_type": link.keyword_type,
                        "reason": "í‚¤ì›Œë“œë¥¼ ë³¸ë¬¸ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ",
                    }
                    for link in unused_links
                ],
            }

            # í”Œë«í¼ë³„ í†µê³„ ê³„ì‚°
            for link in applied_links:
                platform = link.platform
                if (
                    platform
                    not in cost_report["link_analysis"]["link_summary"]["í”Œë«í¼ë³„"]
                ):
                    cost_report["link_analysis"]["link_summary"]["í”Œë«í¼ë³„"][
                        platform
                    ] = 0
                cost_report["link_analysis"]["link_summary"]["í”Œë«í¼ë³„"][platform] += 1
            json_file = (
                project_root / f"data/cost_analysis_report_{safe_kw2}_{timestamp2}.json"
            )
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(cost_report, f, ensure_ascii=False, indent=2)
            alias_json = project_root / f"data/OUTPUT_{safe_kw2}.json"
            with open(alias_json, "w", encoding="utf-8") as f:
                json.dump(cost_report, f, ensure_ascii=False, indent=2)

            print(f"\níŒŒì¼ ìƒì„± ì™„ë£Œ:")
            print(f"   - Step1: {step1_file.name} / ë³„ì¹­: {alias_step1.name}")
            print(f"   - Step2: {step2_file.name} / ë³„ì¹­: {alias_step2.name}")
            print(f"   - Markdown: {md_file.name} / ë³„ì¹­: {alias_md.name}")
            print(f"   - HTML: {html_file.name} / ë³„ì¹­: {alias_html.name}")
            print(f"   - Cost Report: {json_file.name} / ë³„ì¹­: {alias_json.name}")

            print("\n" + "=" * 60)
            print("Enhanced RAG íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            print("=" * 60)
            print(f"í‚¤ì›Œë“œ: {keyword}")
            print(f"ì œëª©: {tk['title']}")
            print(f"ëª¨ë¸: gpt-5-nano + gpt-image-1 (temperature=1.0)")
            print(f"RAG í™œì„±í™”: {'ì˜ˆ' if rag_enabled else 'ì•„ë‹ˆì˜¤'}")
            print(f"ìƒì„± ì‹œê°„: {total_duration:.1f}ì´ˆ")
            print(f"ì„¹ì…˜ ìˆ˜: {len(sections_content)}ê°œ")
            print(f"ìƒì„±ëœ ì´ë¯¸ì§€: {len(images)}ê°œ")
            print(f"ìƒì„±ëœ ì™¸ë¶€ë§í¬: {len(applied_links)}ê°œ")
            if unused_links:
                print(
                    f"   âš ï¸  ë¯¸ì‚¬ìš© ë§í¬: {len(unused_links)}ê°œ (í‚¤ì›Œë“œë¥¼ ë³¸ë¬¸ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ)"
                )
            print(
                f"ì´ ì½˜í…ì¸  ê¸¸ì´: {sum(len(s['content']) for s in sections_content):,}ì"
            )
            # ì´ë¯¸ì§€ ë¹„ìš© í‘œì‹œ
            if len(images) > 0:
                total_image_cost = sum(
                    img["cost_usd"] for img in self.cost_tracker["image_details"]
                )
                print(f"ì´ë¯¸ì§€ ìƒì„± ë¹„ìš©: ${total_image_cost:.3f}")
                print(f"ìƒì„±ëœ ì´ë¯¸ì§€ íŒŒì¼:")
                for key, filename in images.items():
                    print(f"  - {key}: {filename}")

            # ì™¸ë¶€ë§í¬ ì •ë³´ í‘œì‹œ
            if len(applied_links) > 0:
                print(f"ì‹¤ì œ ì ìš©ëœ ì™¸ë¶€ë§í¬:")
                for link in applied_links:
                    print(f"  - {link.anchor_text} â†’ {link.platform}")

            if len(unused_links) > 0:
                print(f"ë¯¸ì‚¬ìš© ì™¸ë¶€ë§í¬:")
                for link in unused_links:
                    print(
                        f"  - {link.anchor_text} â†’ {link.platform} (ë³¸ë¬¸ì—ì„œ í‚¤ì›Œë“œ ì—†ìŒ)"
                    )

            return {
                "success": True,
                "files": {
                    "step1": str(alias_step1),
                    "step2": str(alias_step2),
                    "markdown": str(alias_md),
                    "html": str(alias_html),
                    "cost_report": str(alias_json),
                },
            }

        except Exception as e:
            print(f"\nì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback

            traceback.print_exc()
            return {"success": False, "error": str(e)}


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    keyword = sys.argv[1] if len(sys.argv) > 1 else "ë¶€ìë˜ëŠ” ìŠµê´€"

    pipeline = EnhancedRAGPipeline()
    result = await pipeline.run_complete_pipeline(keyword)

    if result["success"]:
        print(f"\nâœ… íŒŒì´í”„ë¼ì¸ ì„±ê³µ! ìƒì„±ëœ íŒŒì¼ë“¤ì„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        print(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {result['error']}")


if __name__ == "__main__":
    asyncio.run(main())
