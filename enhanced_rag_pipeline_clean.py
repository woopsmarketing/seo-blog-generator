#!/usr/bin/env python3
"""
Enhanced RAG Pipeline for SEO Blog Generation - Clean Version
- Detailed cost analysis
- LSI and longtail keywords
- Structured HTML output
- Token tracking
"""

import sys
import os
import time
import json
import asyncio
import base64
import random
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple, Optional
import re
from openai import OpenAI

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.utils.config import load_config
from src.utils.llm_factory import create_gpt5_nano, LLMFactory, LLMConfig
from src.utils.rag import SimpleRAG
from src.utils.image_optimizer import ImageOptimizer
from src.utils.external_link_builder import ExternalLinkBuilder
from src.utils.wordpress_poster import WordPressPoster, create_wordpress_poster
from src.utils.content_storage import ContentStorage, create_content_storage
from src.utils.internal_link_builder import (
    InternalLinkBuilder,
    create_internal_link_builder,
)
from langchain_core.messages import HumanMessage


class EnhancedRAGPipeline:
    """Enhanced RAG Pipeline with detailed analytics"""

    def __init__(self):
        self.config = load_config()
        self.llm = create_gpt5_nano()
        self.rag = None
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì´ë¯¸ì§€ ìƒì„±ìš©)
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        # ì´ë¯¸ì§€ ìµœì í™” ë„êµ¬ ì´ˆê¸°í™”
        self.image_optimizer = ImageOptimizer()
        # ì™¸ë¶€ë§í¬ ìƒì„± ë„êµ¬ ì´ˆê¸°í™”
        self.external_link_builder = ExternalLinkBuilder()
        # ì›Œë“œí”„ë ˆìŠ¤ í¬ìŠ¤í„° ì´ˆê¸°í™” (ì˜µì…˜)
        self.wordpress_poster = None
        # ì½˜í…ì¸  ì €ì¥ì†Œ ì´ˆê¸°í™” (ì˜µì…˜)
        self.content_storage = None
        # ë‚´ë¶€ë§í¬ ë¹Œë” ì´ˆê¸°í™” (ì˜µì…˜)
        self.internal_link_builder = None
        self.cost_tracker = {
            "total_calls": 0,
            "total_tokens": {"prompt": 0, "completion": 0},
            "total_duration": 0,
            "total_images": 0,  # ì´ë¯¸ì§€ ìƒì„± íšŸìˆ˜ ì¶”ì 
            "step_details": [],
            "image_details": [],  # ì´ë¯¸ì§€ ìƒì„± ë¹„ìš© ì¶”ì 
        }

    def _safe_fragment(self, text: str, max_len: int = 120) -> str:
        """ìœˆë„ìš° í˜¸í™˜ íŒŒì¼ëª… ì¡°ê° ìƒì„± (ê¸ˆì§€ë¬¸ì ì œê±°/ì¹˜í™˜)"""
        frag = re.sub(r"[\\/:*?\"<>|]", "_", text)
        frag = re.sub(r"\s+", "_", frag)
        frag = frag.strip("._")
        if not frag:
            frag = "output"
        return frag[:max_len]

    def setup_rag(self, data_dir: str = "data"):
        """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        data_path = project_root / data_dir
        if data_path.exists():
            self.rag = SimpleRAG(docs_dir=str(data_path))
            self.rag.build()
            return self.rag.vs is not None
        return False

    def track_llm_call(
        self,
        step_name: str,
        prompt_tokens: int,
        completion_tokens: int,
        duration: float,
        output: str,
        purpose: str,
    ):
        """LLM í˜¸ì¶œ ì¶”ì """
        self.cost_tracker["total_calls"] += 1
        self.cost_tracker["total_tokens"]["prompt"] += prompt_tokens
        self.cost_tracker["total_tokens"]["completion"] += completion_tokens
        self.cost_tracker["total_duration"] += duration

        # GPT-5-nano ë¹„ìš© ê³„ì‚° (ì¶”ì •)
        input_cost_per_1k = 0.00005  # $0.05 per 1M tokens = $0.00005 per 1K tokens
        output_cost_per_1k = 0.0004  # $0.40 per 1M tokens = $0.0004 per 1K tokens

        step_cost = (prompt_tokens * input_cost_per_1k / 1000) + (
            completion_tokens * output_cost_per_1k / 1000
        )

        self.cost_tracker["step_details"].append(
            {
                "step": step_name,
                "timestamp": datetime.now().strftime("%H:%M:%S"),
                "duration_seconds": duration,
                "tokens": {"prompt": prompt_tokens, "completion": completion_tokens},
                "estimated_cost_usd": step_cost,
                "purpose": purpose,
                "output_summary": output[:100] + "..." if len(output) > 100 else output,
            }
        )

    async def generate_title_keywords(self, keyword: str) -> Dict[str, Any]:
        """ë‹¨ì¼ í˜¸ì¶œ: ì œëª©, LSI í‚¤ì›Œë“œ, ë¡±í…Œì¼ í‚¤ì›Œë“œ ìƒì„±"""
        start_time = time.time()
        # 1ë‹¨ê³„: í‚¤ì›Œë“œ ë¨¼ì € ìƒì„±
        keywords_prompt = f"""
ë©”ì¸ í‚¤ì›Œë“œ: {keyword}

ì´ ë©”ì¸ í‚¤ì›Œë“œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì•„ë˜ë¥¼ ìƒì„±í•˜ì„¸ìš”:

1) lsi_keywords: ì˜ë¯¸ì ìœ¼ë¡œ ì—°ê´€ëœ LSI í‚¤ì›Œë“œ 5-10ê°œ ë°°ì—´
2) longtail_keywords: êµ¬ì²´ì ì¸ ë¡±í…Œì¼ í‚¤ì›Œë“œ 5-10ê°œ ë°°ì—´

ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì˜ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”:
{{
  "lsi_keywords": ["..."],
  "longtail_keywords": ["..."]
}}
"""
        print("   ğŸ“ 1ë‹¨ê³„: LSI/ë¡±í…Œì¼ í‚¤ì›Œë“œ ìƒì„± ì¤‘...")

        # í‚¤ì›Œë“œ ìƒì„± í˜¸ì¶œ
        messages = [HumanMessage(content=keywords_prompt)]
        keywords_response = self.llm.invoke(messages)

        try:
            import re, json as _json

            m = re.search(r"\{[\s\S]*\}$", keywords_response.content.strip())
            keywords_data = (
                _json.loads(m.group(0)) if m else _json.loads(keywords_response.content)
            )
            lsi_keywords = keywords_data.get("lsi_keywords", [])
            longtail_keywords = keywords_data.get("longtail_keywords", [])
        except Exception:
            lsi_keywords = [f"{keyword} íŒ", f"{keyword} ë°©ë²•"]
            longtail_keywords = [f"{keyword} ì´ˆë³´ ê°€ì´ë“œ"]

        print("   ğŸ“ 2ë‹¨ê³„: ì œëª© ìƒì„± ì¤‘...")

        # 2ë‹¨ê³„: í‚¤ì›Œë“œë“¤ì„ ì¡°í•©í•´ì„œ ì œëª© ìƒì„± (ìµœëŒ€ 3ë²ˆ ì‹œë„)
        max_attempts = 3
        final_title = None

        for attempt in range(max_attempts):
            # 1ë‹¨ê³„: ì œëª© ìƒì„± (ë¨¼ì € LLM í˜¸ì¶œ)
            all_keywords = [keyword] + lsi_keywords[:5] + longtail_keywords[:3]

            title_prompt = f"""
ë©”ì¸ í‚¤ì›Œë“œ: {keyword}
LSI í‚¤ì›Œë“œ: {', '.join(lsi_keywords[:5])}
ë¡±í…Œì¼ í‚¤ì›Œë“œ: {', '.join(longtail_keywords[:3])}

ìœ„ í‚¤ì›Œë“œë“¤ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì¡°í•©í•˜ì—¬ SEO ìµœì í™”ëœ ë¸”ë¡œê·¸ ì œëª©ì„ ë§Œë“œì„¸ìš”.
- ë©”ì¸ í‚¤ì›Œë“œëŠ” ë°˜ë“œì‹œ í¬í•¨
- LSIë‚˜ ë¡±í…Œì¼ í‚¤ì›Œë“œ 1-2ê°œë„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
- 60ì ì´ë‚´
- í´ë¦­ì„ ìœ ë„í•˜ëŠ” ë§¤ë ¥ì ì¸ ì œëª©

ì œëª©ë§Œ ì¶œë ¥í•˜ì„¸ìš” (JSONì´ë‚˜ ë‹¤ë¥¸ í˜•ì‹ ì—†ì´):
"""

            # ì œëª© ìƒì„±
            title_messages = [HumanMessage(content=title_prompt)]
            title_response = self.llm.invoke(title_messages)
            generated_title = title_response.content.strip().strip('"')

            print(f"   ğŸ“ ìƒì„±ëœ ì œëª©: {generated_title}")

            # 2ë‹¨ê³„: ê¸°ì¡´ ì œëª©ë“¤ê³¼ ìœ ì‚¬ë„ ê²€ì‚¬
            avoid_titles = []
            if self.content_storage:
                try:
                    similar_posts = self.content_storage.find_similar_posts(
                        query_text=keyword,
                        k=5,
                        min_similarity_score=0.2,
                        search_titles_only=True,
                    )
                    avoid_titles = [post["metadata"]["title"] for post in similar_posts]
                    if avoid_titles:
                        print(
                            f"   ğŸ“‹ ê¸°ì¡´ ì œëª© {len(avoid_titles)}ê°œì™€ ìœ ì‚¬ë„ ê²€ì‚¬ ì¤‘..."
                        )
                except Exception as e:
                    print(f"   âš ï¸ ì œëª© ì¤‘ë³µ ê²€ì‚¬ ì‹¤íŒ¨: {e}")

            # 3ë‹¨ê³„: ìœ ì‚¬ë„ ê²€ì‚¬ ìˆ˜í–‰
            if avoid_titles:
                is_similar = False
                for existing_title in avoid_titles:
                    # ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê²€ì‚¬ (í‚¤ì›Œë“œ ê²¹ì¹¨ ë¹„ìœ¨)
                    title_words = set(generated_title.lower().split())
                    existing_words = set(existing_title.lower().split())

                    intersection = len(title_words & existing_words)
                    union = len(title_words | existing_words)
                    similarity = intersection / union if union > 0 else 0

                    if similarity > 0.6:  # 60% ì´ìƒ ìœ ì‚¬í•˜ë©´
                        is_similar = True
                        print(
                            f"   âš ï¸ ìœ ì‚¬í•œ ì œëª© ë°œê²¬ (ìœ ì‚¬ë„ {similarity:.1%}): {existing_title}"
                        )
                        break

                if not is_similar:
                    print(f"   âœ… ê³ ìœ í•œ ì œëª© ìƒì„± ì™„ë£Œ")
                    final_title = generated_title
                    break
                elif attempt < max_attempts - 1:
                    print(f"   ğŸ”„ ì œëª© ì¬ìƒì„± ì‹œë„ {attempt + 2}/{max_attempts}")
            else:
                final_title = generated_title
                break

        if not final_title:
            final_title = generated_title  # ë§ˆì§€ë§‰ ì‹œë„ ê²°ê³¼ ì‚¬ìš©

        # ë¹„ìš© ì¶”ì 
        duration = time.time() - start_time
        prompt_tokens = int(len(keywords_prompt.split()) * 1.3) + int(
            len(title_prompt.split()) * 1.3
        )
        completion_tokens = int(len(keywords_response.content.split()) * 1.3) + int(
            len(title_response.content.split()) * 1.3
        )

        self.track_llm_call(
            "generate_title_keywords",
            prompt_tokens,
            completion_tokens,
            duration,
            f"í‚¤ì›Œë“œ: {len(lsi_keywords + longtail_keywords)}ê°œ, ì œëª©: {final_title}",
            "í‚¤ì›Œë“œ ë¨¼ì € ìƒì„± â†’ ì œëª© ì¡°í•© ìƒì„±",
        )

        return {
            "title": final_title,
            "lsi_keywords": lsi_keywords,
            "longtail_keywords": longtail_keywords,
            "notes": f"ë©”ì¸ í‚¤ì›Œë“œ '{keyword}' ê¸°ë°˜ìœ¼ë¡œ LSI/ë¡±í…Œì¼ í‚¤ì›Œë“œë¥¼ ë¨¼ì € ìƒì„±í•œ í›„ ì œëª©ì„ ì¡°í•© ìƒì„±",
        }

    async def generate_structure_json(
        self, title: str, keyword: str, lsi: List[str], longtail: List[str]
    ) -> Dict[str, Any]:
        """ë¸”ë¡œê·¸ êµ¬ì¡° JSON ìƒì„± (7-10ê°œ H2 ì„¹ì…˜, ê°œìš”+ë§ˆë¬´ë¦¬+FAQ í¬í•¨)"""
        import random

        start_time = time.time()
        target_sections = random.randint(7, 10)
        joined = ", ".join((lsi or [])[:5] + (longtail or [])[:3])
        prompt = f"""
ì œëª©: {title}
í‚¤ì›Œë“œ: {keyword}
ì—°ê´€ í‚¤ì›Œë“œ: {joined}

ì•„ë˜ ì¡°ê±´ìœ¼ë¡œ ë¸”ë¡œê·¸ ë¬¸ì„œ êµ¬ì¡°ë¥¼ JSONìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”.
- H2 ì„¹ì…˜ ìˆ˜: ì •í™•íˆ {target_sections}ê°œ
- ì²« ë²ˆì§¸ H2ëŠ” ë°˜ë“œì‹œ 'ê°œìš”', 'ì†Œê°œ', 'ì‹œì‘í•˜ê¸°' ì¤‘ í•˜ë‚˜ì˜ ì„±ê²©ì„ ê°€ì§„ ë„ì… ì„¹ì…˜ì´ì–´ì•¼ í•©ë‹ˆë‹¤ (ììœ ë¡­ê²Œ í‘œí˜„ ê°€ëŠ¥)
- ë§ˆì§€ë§‰ì—ì„œ ë‘ ë²ˆì§¸ H2ëŠ” 'ì •ë¦¬ì™€ ë§ˆë¬´ë¦¬', 'ìš”ì•½ê³¼ ê²°ë¡ ', 'í•µì‹¬ í¬ì¸íŠ¸' ë“± ì „ì²´ ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” ì„¹ì…˜ì´ì–´ì•¼ í•©ë‹ˆë‹¤
- ë§ˆì§€ë§‰ H2ëŠ” ë°˜ë“œì‹œ 'ìì£¼ ë¬»ëŠ” ì§ˆë¬¸', 'FAQ', 'ê¶ê¸ˆí•œ ì ë“¤' ë“± ì§ˆë¬¸-ë‹µë³€ í˜•íƒœì˜ ì„¹ì…˜ì´ì–´ì•¼ í•©ë‹ˆë‹¤
- ê° H2ë§ˆë‹¤ H3/H4ëŠ” ìœ ë™ì ìœ¼ë¡œ 0ê°œ ì´ìƒ í¬í•¨ ê°€ëŠ¥
- í•œêµ­ì–´ ì œëª© ì‚¬ìš©, í‚¤ì›Œë“œëŠ” ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨

ë°˜í™˜ í˜•ì‹ ì˜ˆì‹œ:
{{
  "title": "{title}",
  "sections": [
    {{
      "h2": "ì„¹ì…˜ ì œëª©",
      "h3": ["ì†Œì œëª©1", "ì†Œì œëª©2"],
      "h4_map": {{"ì†Œì œëª©1": ["í•­ëª©1", "í•­ëª©2"]}}
    }}
  ]
}}
ë°˜ë“œì‹œ ìœ„ì˜ JSON ìŠ¤í‚¤ë§ˆë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""
        # êµ¬ì¡° ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ RAG ê²€ìƒ‰ ì œê±° - ì œëª© ì¤‘ë³µ ê²€ì‚¬ë¡œ ì¶©ë¶„í•¨

        messages = [HumanMessage(content=prompt)]
        response = self.llm.invoke(messages)
        duration = time.time() - start_time
        prompt_tokens = int(len(prompt.split()) * 1.3)
        completion_tokens = int(len(response.content.split()) * 1.3)

        self.track_llm_call(
            "structure_generation",
            prompt_tokens,
            completion_tokens,
            duration,
            response.content,
            "7-10ê°œì˜ H2ì™€ ìœ ë™ H3/H4 êµ¬ì¡° JSON (ê°œìš”+ë§ˆë¬´ë¦¬+FAQ í¬í•¨)",
        )

        try:
            import re, json as _json

            m = re.search(r"\{[\s\S]*\}$", response.content.strip())
            data = _json.loads(m.group(0)) if m else _json.loads(response.content)
            return data
        except Exception:
            return {"title": title, "sections": []}

    async def summarize_previous(self, text: str) -> str:
        """ì´ì „ ì„¹ì…˜ ë‚´ìš© ìš”ì•½"""
        prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ë§Œ 2-3ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ ìš”ì•½:
---
{text}
---
ìš”ì•½:
"""
        messages = [HumanMessage(content=prompt)]
        response = self.llm.invoke(messages)
        return response.content.strip()

    async def generate_image(self, prompt: str, purpose: str) -> Optional[str]:
        """ì´ë¯¸ì§€ ìƒì„± (gpt-image-1 ëª¨ë¸ ì‚¬ìš©)

        Args:
            prompt: ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ ì˜ë¬¸ í”„ë¡¬í”„íŠ¸
            purpose: ì´ë¯¸ì§€ ìš©ë„ (cost trackingìš©)

        Returns:
            ìƒì„±ëœ ì´ë¯¸ì§€ì˜ base64 ë¬¸ìì—´ ë˜ëŠ” None
        """
        try:
            start_time = time.time()

            # OpenAI Image API í˜¸ì¶œ (gpt-image-1ì€ í•­ìƒ base64ë¡œ ë°˜í™˜)
            response = self.openai_client.images.generate(
                model="gpt-image-1",
                prompt=prompt,
                quality="low",  # ì €í’ˆì§ˆ (ê°€ê²© íš¨ìœ¨ì„±)
                size="1024x1024",  # í‘œì¤€ ì‚¬ì´ì¦ˆ
                n=1,  # 1ê°œ ì´ë¯¸ì§€
            )

            duration = time.time() - start_time

            # ë¹„ìš© ê³„ì‚° (gpt-image-1 low quality 1024x1024: $0.011)
            image_cost = 0.011

            # ì´ë¯¸ì§€ ìƒì„± ì¶”ì 
            self.cost_tracker["total_images"] += 1
            self.cost_tracker["image_details"].append(
                {
                    "purpose": purpose,
                    "timestamp": datetime.now().strftime("%H:%M:%S"),
                    "duration_seconds": duration,
                    "prompt": prompt[:100] + "..." if len(prompt) > 100 else prompt,
                    "cost_usd": image_cost,
                    "model": "gpt-image-1",
                    "quality": "low",
                    "size": "1024x1024",
                }
            )

            # base64 ì´ë¯¸ì§€ ë°ì´í„° ë°˜í™˜ (gpt-image-1ì€ í•­ìƒ b64_json í˜•íƒœ)
            return response.data[0].b64_json

        except Exception as e:
            print(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨ ({purpose}): {e}")
            return None

    def save_image_from_base64(
        self, b64_data: str, file_path: Path, optimize: bool = True
    ) -> bool:
        """base64 ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥ ë° ìµœì í™”

        Args:
            b64_data: base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë°ì´í„°
            file_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
            optimize: ì´ë¯¸ì§€ ìµœì í™” ì—¬ë¶€

        Returns:
            ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # base64 ë””ì½”ë”©í•˜ì—¬ PNG íŒŒì¼ë¡œ ì €ì¥
            image_data = base64.b64decode(b64_data)
            file_path.parent.mkdir(exist_ok=True)

            with open(file_path, "wb") as f:
                f.write(image_data)

            # ì´ë¯¸ì§€ ìµœì í™” (ì˜µì…˜)
            if optimize:
                # ë¸”ë¡œê·¸ìš© ìµœì í™”: 512x512 ì´í•˜, 50KB ì´í•˜ë¡œ ì••ì¶•
                optimization_result = self.image_optimizer.optimize_for_web(
                    file_path,
                    max_size=(512, 512),
                    target_file_size_kb=50,
                    quality_range=(70, 90),
                )

                if optimization_result["success"]:
                    reduction = optimization_result["size_reduction_percent"]
                    print(
                        f"     ğŸ“‰ ì´ë¯¸ì§€ ìµœì í™”: {optimization_result['file_size_change']} ({reduction}% ê°ì†Œ)"
                    )
                else:
                    print(
                        f"     âš ï¸ ì´ë¯¸ì§€ ìµœì í™” ì‹¤íŒ¨: {optimization_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
                    )

            return True
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    async def generate_and_save_images(
        self,
        title: str,
        sections: List[Dict],
        keyword: str,
        lsi_keywords: List[str] = None,
        longtail_keywords: List[str] = None,
    ) -> Dict[str, str]:
        """ë©”ì¸ ë° ì„¹ì…˜ë³„ ì´ë¯¸ì§€ ìƒì„± ë° ì €ì¥

        Args:
            title: ë¸”ë¡œê·¸ ì œëª©
            sections: ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸
            keyword: ë©”ì¸ í‚¤ì›Œë“œ
            lsi_keywords: LSI í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            longtail_keywords: ë¡±í…Œì¼ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì´ë¯¸ì§€ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬ {"main": "path", "section_1": "path", ...}
        """
        images = {}
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_keyword = self._safe_fragment(keyword)
        images_dir = project_root / "data" / "images"
        images_dir.mkdir(parents=True, exist_ok=True)

        # 1. ë©”ì¸ ì´ë¯¸ì§€ ìƒì„± (100% í™•ë¥ )
        print("4. ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
        main_prompt = f"Create a professional diagram or infographic about '{title}'. Chart, concept diagram, or infographic style. No text or words in the image. Clean, modern design."

        main_image_data = await self.generate_image(
            main_prompt, f"ë©”ì¸ ì´ë¯¸ì§€: {title}"
        )
        if main_image_data:
            main_image_path = images_dir / f"main_{safe_keyword}_{timestamp}.png"
            if self.save_image_from_base64(
                main_image_data, main_image_path, optimize=True
            ):
                images["main"] = str(main_image_path)
                print(f"   âœ… ë©”ì¸ ì´ë¯¸ì§€ ìƒì„±: {main_image_path.name}")

        # 2. ì„¹ì…˜ë³„ ì´ë¯¸ì§€ ìƒì„± (33% í™•ë¥ )
        for i, section in enumerate(sections):
            if random.random() <= 0.33:  # 33% í™•ë¥ 
                section_title = section.get("h2_title", f"ì„¹ì…˜ {i+1}")
                section_prompt = f"Create a diagram or concept illustration about '{section_title}'. Professional infographic style. No text or words. Clean design."

                section_image_data = await self.generate_image(
                    section_prompt, f"ì„¹ì…˜ ì´ë¯¸ì§€: {section_title}"
                )
                if section_image_data:
                    section_image_path = (
                        images_dir / f"section_{i+1}_{safe_keyword}_{timestamp}.png"
                    )
                    if self.save_image_from_base64(
                        section_image_data, section_image_path, optimize=True
                    ):
                        images[f"section_{i+1}"] = str(section_image_path)
                        print(
                            f"   âœ… ì„¹ì…˜ {i+1} ì´ë¯¸ì§€ ìƒì„±: {section_image_path.name}"
                        )

        return images

    def generate_table_of_contents(self, sections_content: List[Dict]) -> str:
        """H2 ê¸°ë°˜ ëª©ì°¨ ìƒì„± (ì•µì»¤ ë§í¬ í¬í•¨, í•µì‹¬ ìš©ì–´ ì •ë¦¬ í¬í•¨)"""
        toc_lines = ["## ğŸ“š ëª©ì°¨\n"]

        # ì²« ë²ˆì§¸: í•µì‹¬ ìš©ì–´ ì •ë¦¬
        toc_lines.append("1. [í•µì‹¬ ìš©ì–´ ì •ë¦¬](#í•µì‹¬-ìš©ì–´-ì •ë¦¬)")

        # ë‚˜ë¨¸ì§€ ì„¹ì…˜ë“¤ (ë²ˆí˜¸ +1)
        for i, section in enumerate(sections_content, 2):  # 2ë¶€í„° ì‹œì‘
            h2_title = section.get("h2_title", f"ì„¹ì…˜ {i}")
            # ë§ˆí¬ë‹¤ìš´ ì•µì»¤ ë§í¬ ìƒì„± (í•œê¸€ -> ì˜ì–´, ê³µë°± -> í•˜ì´í”ˆ)
            anchor_id = (
                h2_title.lower()
                .replace(" ", "-")
                .replace(":", "")
                .replace("?", "")
                .replace("!", "")
            )
            # í•œê¸€ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜ íŠ¹ìˆ˜ë¬¸ìë§Œ ì œê±°
            anchor_id = (
                h2_title.replace(" ", "-")
                .replace(":", "")
                .replace("?", "")
                .replace("!", "")
                .replace(",", "")
                .replace(".", "")
            )
            toc_lines.append(f"{i}. [{h2_title}](#{anchor_id})")

        return "\n".join(toc_lines) + "\n"

    async def extract_and_explain_terms(self, full_content: str, keyword: str) -> str:
        """ì½˜í…ì¸ ì—ì„œ ì–´ë ¤ìš´ ìš©ì–´ ì¶”ì¶œ ë° ì„¤ëª… ìƒì„±"""
        start_time = time.time()

        prompt = f"""
ë‹¤ìŒì€ '{keyword}' ì£¼ì œì˜ ë¸”ë¡œê·¸ ì½˜í…ì¸ ì…ë‹ˆë‹¤.
ì´ˆë³´ìë‚˜ ì¤‘ê¸‰ìê°€ ì½ì„ ë•Œ ì´í•´í•˜ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆëŠ” ì „ë¬¸ ìš©ì–´ë¥¼ 5-8ê°œ ì„ ë³„í•˜ê³ , ê°ê°ì„ í•œ ì¤„ë¡œ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

=== ë¸”ë¡œê·¸ ì½˜í…ì¸  ===
{full_content[:4000]}  # í† í° ì œí•œì„ ìœ„í•´ ì¼ë¶€ë§Œ ì‚¬ìš©

=== ì‘ì—… ì§€ì‹œ ===
1. ìœ„ ì½˜í…ì¸ ì—ì„œ ì´ˆë³´ìê°€ ëª¨ë¥¼ ë§Œí•œ ì „ë¬¸ ìš©ì–´ë¥¼ ì„ ë³„í•˜ì„¸ìš”
2. ê° ìš©ì–´ë¥¼ í•œ ì¤„(25ì ì´ë‚´)ë¡œ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”
3. ì¤‘ë³µ ìš©ì–´ë‚˜ ë„ˆë¬´ ì‰¬ìš´ ìš©ì–´ëŠ” ì œì™¸í•˜ì„¸ìš”
4. ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”

=== ì¶œë ¥ í˜•ì‹ (ì˜ˆì‹œ) ===
í¬ë¡¤ë§: ê²€ìƒ‰ì—”ì§„ì´ ì›¹í˜ì´ì§€ë¥¼ ì½ì–´ê°€ëŠ” ê³¼ì •
ë°±ë§í¬: ë‹¤ë¥¸ ì‚¬ì´íŠ¸ì—ì„œ ë‚´ ì‚¬ì´íŠ¸ë¡œ ì—°ê²°ë˜ëŠ” ë§í¬
ë©”íƒ€íƒœê·¸: ê²€ìƒ‰ì—”ì§„ì—ê²Œ í˜ì´ì§€ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ëŠ” ì½”ë“œ
ì¸ë±ì‹±: ê²€ìƒ‰ì—”ì§„ì´ í˜ì´ì§€ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” ì‘ì—…
ì•µì»¤í…ìŠ¤íŠ¸: ë§í¬ì— í‘œì‹œë˜ëŠ” í´ë¦­ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸

ìœ„ í˜•ì‹ìœ¼ë¡œ ìš©ì–´ì™€ ì„¤ëª…ë§Œ ì¶œë ¥í•˜ì„¸ìš”:
"""

        messages = [HumanMessage(content=prompt)]
        response = self.llm.invoke(messages)

        # ìš©ì–´ ì„¹ì…˜ í¬ë§·íŒ… (ì•™ì»¤ ID í¬í•¨)
        terms_section = '<h2 id="terms-section">ğŸ“– í•µì‹¬ ìš©ì–´ ì •ë¦¬</h2>\n\n'
        terms_section += "ë³¸ë¬¸ì„ ì½ê¸° ì „ì— ì•Œì•„ë‘ë©´ ì¢‹ì€ ìš©ì–´ë“¤ì…ë‹ˆë‹¤.\n\n"

        # LLM ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ìš©ì–´ ì •ë¦¬ (ê°œì„ ëœ íŒŒì‹±)
        response_text = response.content.strip()
        lines = response_text.split("\n")
        terms_found = 0

        print(f"   ğŸ” LLM ì‘ë‹µ ê¸¸ì´: {len(response_text)}ì")
        print(f"   ğŸ“ ì‘ë‹µ ë¼ì¸ ìˆ˜: {len(lines)}ê°œ")
        print(f"   ğŸ“„ LLM ì›ë³¸ ì‘ë‹µ:")
        print(f"   {response_text}")
        print("   " + "=" * 50)

        for line in lines:
            line = line.strip()
            # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
            if any(
                skip in line.lower()
                for skip in ["ì¶œë ¥ í˜•ì‹", "ì˜ˆì‹œ", "ì‘ì—… ì§€ì‹œ", "ë¸”ë¡œê·¸ ì½˜í…ì¸ ", "==="]
            ):
                continue

            if ":" in line and len(line) > 8:  # ìµœì†Œ ê¸¸ì´ ì²´í¬ ê°•í™”
                try:
                    # ì½œë¡ ìœ¼ë¡œ ë¶„í• 
                    parts = line.split(":", 1)
                    if len(parts) == 2:
                        term = (
                            parts[0]
                            .strip()
                            .replace("**", "")
                            .replace("-", "")
                            .replace("*", "")
                            .strip()
                        )
                        explanation = parts[1].strip()

                        # ìœ íš¨ì„± ê²€ì‚¬
                        if (
                            term
                            and explanation
                            and len(term) > 1
                            and len(explanation) > 5
                            and not term.isdigit()
                        ):  # ìˆ«ìë§Œì¸ ìš©ì–´ ì œì™¸
                            terms_section += f"**{term}**: {explanation}\n\n"
                            terms_found += 1
                            print(f"   âœ… ìš©ì–´ ì¶”ê°€: {term}")
                except Exception as e:
                    print(f"   âš ï¸ íŒŒì‹± ì˜¤ë¥˜: {line[:50]}...")
                    continue

        print(f"   ğŸ“Š ì´ ì¶”ì¶œëœ ìš©ì–´: {terms_found}ê°œ")

        # ìš©ì–´ê°€ í•˜ë‚˜ë„ ì¶”ì¶œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ê¸°ë³¸ ìš©ì–´ ì¶”ê°€
        if terms_found == 0:
            print("   ğŸ”„ ê¸°ë³¸ ìš©ì–´ë¡œ ëŒ€ì²´")
            terms_section += f"**{keyword}**: ì´ ê¸€ì˜ ì£¼ìš” ì£¼ì œì…ë‹ˆë‹¤\n\n"
            terms_section += (
                f"**SEO**: ê²€ìƒ‰ì—”ì§„ ìµœì í™”ë¡œ ì›¹ì‚¬ì´íŠ¸ ë…¸ì¶œì„ ë†’ì´ëŠ” ê¸°ë²•\n\n"
            )
            terms_section += f"**í‚¤ì›Œë“œ**: ê²€ìƒ‰í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ë‚˜ ë¬¸êµ¬\n\n"

        duration = time.time() - start_time
        self.track_llm_call(
            "extract_terms",
            int(len(prompt.split()) * 1.3),
            int(len(response.content.split()) * 1.3),
            duration,
            f"ìš©ì–´ {terms_found}ê°œ ì¶”ì¶œ",
            "ì–´ë ¤ìš´ ìš©ì–´ ì¶”ì¶œ ë° ì„¤ëª…",
        )

        print(f"   ğŸ“ ìµœì¢… terms_section ê¸¸ì´: {len(terms_section)}ì")
        print(f"   ğŸ“„ ìµœì¢… terms_section ë¯¸ë¦¬ë³´ê¸°:")
        print(f"   {terms_section[:300]}...")

        return terms_section

    def cleanup_images_folder(self):
        """ì´ë¯¸ì§€ í´ë” ì •ë¦¬ (ìƒì„±ëœ ì´ë¯¸ì§€ë“¤ ì‚­ì œ)"""
        try:
            images_dir = project_root / "data" / "images"
            if not images_dir.exists():
                return

            # .png íŒŒì¼ë“¤ë§Œ ì‚­ì œ (ë‹¤ë¥¸ íŒŒì¼ì€ ë³´ì¡´)
            deleted_count = 0
            for image_file in images_dir.glob("*.png"):
                try:
                    image_file.unlink()
                    deleted_count += 1
                except Exception as e:
                    print(f"   âš ï¸ ì´ë¯¸ì§€ ì‚­ì œ ì‹¤íŒ¨: {image_file.name} - {e}")

            if deleted_count > 0:
                print(f"   ğŸ—‘ï¸ ì´ë¯¸ì§€ í´ë” ì •ë¦¬ ì™„ë£Œ: {deleted_count}ê°œ íŒŒì¼ ì‚­ì œ")

        except Exception as e:
            print(f"   âš ï¸ ì´ë¯¸ì§€ í´ë” ì •ë¦¬ ì‹¤íŒ¨: {e}")

    async def generate_section_with_context(
        self,
        idx: int,
        total: int,
        section: Dict[str, Any],
        keyword: str,
        title: str,
        full_structure_json: Dict[str, Any],
        prev_summary: str = "",
        next_h2: str = "",
        lsi_keywords: List[str] = None,
        longtail_keywords: List[str] = None,
    ) -> Tuple[str, List[str]]:
        """ì„¹ì…˜ë³„ ì½˜í…ì¸  ìƒì„± (ì»¨í…ìŠ¤íŠ¸ì™€ í‹°ì € í¬í•¨, ì‚¬ìš©ëœ í‚¤ì›Œë“œ ë°˜í™˜)"""
        start_time = time.time()
        structure_str = json.dumps(full_structure_json, ensure_ascii=False)
        ctx = f"ì´ì „ ì„¹ì…˜ ìš”ì•½: {prev_summary}\n" if prev_summary else ""

        # í‹°ì € ë¬¸ì¥ ê°€ì´ë“œ (ëª…ì‹œì  í‘œí˜„ ê¸ˆì§€)
        teaser = (
            f"ë§ˆì§€ë§‰ ë¬¸ì¥ì€ '{next_h2}'ì™€ ì£¼ì œê°€ ë§ë‹¿ì•„ ìˆìŒì„ ë…ìê°€ ì•”ì‹œì ìœ¼ë¡œ ëŠë¼ë„ë¡, êµ¬ì²´ì  ì •ë³´ í•œ ì¡°ê°ìœ¼ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”. \n- ê¸ˆì§€ í‘œí˜„: 'ë‹¤ìŒ', 'ë‹¤ìŒ ì„¹ì…˜', 'ë‹¤ìŒ ì±•í„°', 'ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§‘ë‹ˆë‹¤'"
            if next_h2
            else ""
        )

        # ê¸¸ì´ ì •ì±…: 1ì„¹ì…˜ 300ì ë‚´ì™¸, ê·¸ ì™¸ 500-800ì
        length_rule = "ë¶„ëŸ‰: ì•½ 300ì" if idx == 1 else "ë¶„ëŸ‰: 500-800ì"

        # LSI/ë¡±í…Œì¼ í‚¤ì›Œë“œë¥¼ ì„¹ì…˜ë³„ë¡œ ë§¤ë²ˆ ìƒˆë¡œ ëœë¤ ì„ íƒ (0-1ê°œì”©)
        import random

        section_keywords = []

        # LSI í‚¤ì›Œë“œì—ì„œ 0-1ê°œ ëœë¤ ì„ íƒ (ë§¤ ì„¹ì…˜ë§ˆë‹¤)
        if lsi_keywords and random.random() < 0.6:  # 60% í™•ë¥ ë¡œ LSI í‚¤ì›Œë“œ í¬í•¨
            selected_lsi = random.choice(lsi_keywords)
            section_keywords.append(selected_lsi)

        # ë¡±í…Œì¼ í‚¤ì›Œë“œì—ì„œ 0-1ê°œ ëœë¤ ì„ íƒ (ë§¤ ì„¹ì…˜ë§ˆë‹¤)
        if longtail_keywords and random.random() < 0.4:  # 40% í™•ë¥ ë¡œ ë¡±í…Œì¼ í‚¤ì›Œë“œ í¬í•¨
            selected_longtail = random.choice(longtail_keywords)
            section_keywords.append(selected_longtail)

        # í‚¤ì›Œë“œ ì •ë³´ êµ¬ì„±
        keywords_info = f"ì£¼ìš” í‚¤ì›Œë“œ: {keyword}"
        if section_keywords:
            keywords_info += (
                f"\nì„¹ì…˜ ê´€ë ¨ í‚¤ì›Œë“œ (ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨): {', '.join(section_keywords)}"
            )

        prompt = f"""
ë¬¸ì„œ ì œëª©: {title}
{keywords_info}
ì „ì²´ ë¬¸ì„œ êµ¬ì¡°(JSON): {structure_str}
í˜„ì¬ ì„¹ì…˜: {idx}/{total} - H2: {section.get('h2')}
{ctx}
ìš”êµ¬ì‚¬í•­:
1) í•œêµ­ì–´ ìì—°ìŠ¤ëŸ¬ìš´ ë³¸ë¬¸
2) H3 ì†Œì œëª© 2-3ê°œ í¬í•¨ ê°€ëŠ¥ (ìˆë‹¤ë©´ Markdown ### ì‚¬ìš©)
3) ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ì¸ ë‚´ìš©
4) {length_rule}
5) {teaser}
6) ì¶œë ¥ì€ "ë³¸ë¬¸ë§Œ" ì‘ì„±. ì„¹ì…˜ ì œëª©ì´ë‚˜ H2(##)ë¥¼ ì¶œë ¥í•˜ì§€ ë§ ê²ƒ. 'ì„¹ì…˜ ë³¸ë¬¸:' ê°™ì€ ì•ˆë‚´ ë¬¸êµ¬ë„ ê¸ˆì§€.
7) ì²« ì¤„ì— ì„¹ì…˜ ì œëª©ì„ ë°˜ë³µí•˜ì§€ ë§ ê²ƒ. í•„ìš” ì‹œ H3(###)ë¶€í„° ì‹œì‘.
8) í‘œ í˜•íƒœë¡œ í‘œí˜„í•˜ë©´ ë” íš¨ê³¼ì ì¸ ë‚´ìš©ì´ ìˆë‹¤ë©´ Markdown í‘œ ë¬¸ë²• ì‚¬ìš©:
   - ë¹„êµí‘œ: | í•­ëª© | ì„¤ëª… | ë¹„ê³  |
   - ë‹¨ê³„ë³„ í‘œ: | ë‹¨ê³„ | ë‚´ìš© | íŒ |
   - íŠ¹ì§•í‘œ: | íŠ¹ì§• | ì¥ì  | ë‹¨ì  |
   - ì˜ˆì‹œ: | êµ¬ë¶„ | ë°©ë²• | íš¨ê³¼ |
   - ë„êµ¬ ë¹„êµ: | ë„êµ¬ëª… | ì¥ì  | ë‹¨ì  | ê°€ê²© |
   - ë‹¨ê³„ë³„ ê°€ì´ë“œ: | ë‹¨ê³„ | ì„¤ëª… | ì£¼ì˜ì‚¬í•­ |
   - íŒ ì •ë¦¬: | ìƒí™© | í•´ê²°ë°©ë²• | íš¨ê³¼ |

ë³¸ë¬¸ ì¶œë ¥ ì‹œì‘:
"""
        # RAG ê²€ìƒ‰ ì œê±° - ë…ì°½ì ì¸ ì½˜í…ì¸  ìƒì„±ì„ ìœ„í•´
        # if self.rag and self.rag.vs:
        #     rag_context = self.rag.query(f"{keyword} {section.get('h2','')} ë°°ê²½", k=2)
        #     prompt = f"{rag_context}\n\n{prompt}"

        messages = [HumanMessage(content=prompt)]
        response = self.llm.invoke(messages)
        duration = time.time() - start_time
        prompt_tokens = int(len(prompt.split()) * 1.3)
        completion_tokens = int(len(response.content.split()) * 1.3)

        self.track_llm_call(
            f"section_{idx}",
            prompt_tokens,
            completion_tokens,
            duration,
            response.content,
            f"ì„¹ì…˜ {idx} ë³¸ë¬¸ ìƒì„±",
        )

        return response.content.strip(), section_keywords

    def _sanitize_section_content(self, h2_title: str, content: str) -> str:
        """ëª¨ë¸ ì‘ë‹µì—ì„œ ì¤‘ë³µ H2/ì•ˆë‚´ë¬¸ ë“±ì„ ì œê±°í•˜ì—¬ ê¹”ë”í•œ ë³¸ë¬¸ë§Œ ë‚¨ê¸´ë‹¤."""
        lines = [ln.rstrip() for ln in content.strip().split("\n")]
        sanitized: list[str] = []
        skip_prefixes = {
            "ì„¹ì…˜ ë³¸ë¬¸:",
            "ë³¸ë¬¸:",
            "ë³¸ë¬¸ ì¶œë ¥:",
            "ë³¸ë¬¸ ì¶œë ¥ ì‹œì‘:",
            "ë‚´ìš©:",
        }

        for i, ln in enumerate(lines):
            # ì²« ë¶€ë¶„ì—ì„œë§Œ ê°•ì œ ì œê±° ê·œì¹™ ì ìš©
            if i < 5:
                # 'ì„¹ì…˜ ë³¸ë¬¸:' ë¥˜ ì•ˆë‚´ë¬¸ ì œê±°
                if any(ln.strip().startswith(p) for p in skip_prefixes):
                    continue
                # ì¤‘ë³µ H2 ì œê±° (## ...)
                if ln.strip().startswith("## "):
                    continue
                # ì„¹ì…˜ ì œëª© ë°˜ë³µ í…ìŠ¤íŠ¸ ì œê±° (ì œëª©ë§Œ ë‹¨ë… ë¼ì¸)
                if ln.strip() == h2_title.strip():
                    continue
            sanitized.append(ln)

        # ì•ë’¤ ê³µë°± ì •ë¦¬ ë° ì—°ì† ë¹ˆ ì¤„ ì¶•ì†Œ
        out = "\n".join(sanitized).strip()
        out = re.sub(r"\n\s*\n\s*\n+", "\n\n", out)
        return out

    def create_markdown(
        self,
        title: str,
        keyword: str,
        sections_content: List[Dict[str, Any]],
        keywords: Dict[str, List[str]],
        images: Optional[Dict[str, str]] = None,
        table_of_contents: str = "",
        terms_section: str = "",
    ) -> str:
        """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ìƒì„± (ëª©ì°¨, ìš©ì–´ ì •ë¦¬, ì´ë¯¸ì§€ í¬í•¨)"""
        md_content = f"# {title}\n\n"
        # 1. ëª©ì°¨ ì¶”ê°€ (ìµœìƒë‹¨)
        if table_of_contents:
            md_content += table_of_contents + "\n"

        # 2. ë©”ì¸ ì´ë¯¸ì§€ ì¶”ê°€ (ëª©ì°¨ ë‹¤ìŒ)
        if images and "main" in images:
            md_content += f'![{title}]({images["main"]})\n\n'

        # 3. ìš©ì–´ ì •ë¦¬ ì¶”ê°€ (ì´ë¯¸ì§€ ë‹¤ìŒ)
        print(f"   ğŸ”§ create_markdownì—ì„œ terms_section ê¸¸ì´: {len(terms_section)}ì")
        if terms_section:
            print(f"   âœ… terms_sectionì„ ë§ˆí¬ë‹¤ìš´ì— ì¶”ê°€")
            md_content += terms_section + "\n"
        else:
            print(f"   âŒ terms_sectionì´ ë¹„ì–´ìˆìŒ!")

        # 4. ë³¸ë¬¸ ì„¹ì…˜ë“¤ (ë§ˆí¬ë‹¤ìš´ í—¤ë”ë¡œ ìƒì„±, HTML ë³€í™˜ê¸°ì—ì„œ ID ì¶”ê°€)
        for i, section in enumerate(sections_content):
            # ë§ˆí¬ë‹¤ìš´ H2 í—¤ë”ë¡œ ìƒì„±
            md_content += f'## {section["h2_title"]}\n\n'

            # ì„¹ì…˜ ì´ë¯¸ì§€ ì¶”ê°€ (20% í™•ë¥ ë¡œ)
            section_image_key = f"section_{i+1}"
            if images and section_image_key in images:
                md_content += (
                    f'![{section["h2_title"]}]({images[section_image_key]})\n\n'
                )

            md_content += f"{section['content']}\n\n"

        return md_content

    def create_cost_analysis_report(
        self,
        keyword: str,
        title: str,
        keywords: Dict,
        sections_content: List,
        total_duration: float,
    ) -> Dict[str, Any]:
        """ìƒì„¸í•œ ë¹„ìš© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        # í…ìŠ¤íŠ¸ ìƒì„± ë¹„ìš©
        text_cost = sum(
            step["estimated_cost_usd"] for step in self.cost_tracker["step_details"]
        )
        # ì´ë¯¸ì§€ ìƒì„± ë¹„ìš©
        image_cost = sum(img["cost_usd"] for img in self.cost_tracker["image_details"])
        total_cost = text_cost + image_cost

        total_tokens = (
            self.cost_tracker["total_tokens"]["prompt"]
            + self.cost_tracker["total_tokens"]["completion"]
        )

        report = {
            "report_info": {
                "keyword": keyword,
                "title": title,
                "generation_date": datetime.now().isoformat(),
                "report_version": "2.0 - Enhanced RAG Pipeline",
            },
            "cost_analysis": {
                "pipeline_summary": {
                    "model_used": "gpt-5-nano + gpt-image-1",
                    "total_duration_seconds": round(total_duration, 1),
                    "sections_generated": len(sections_content),
                    "images_generated": self.cost_tracker["total_images"],
                    "total_estimated_cost_usd": round(total_cost, 6),
                    "text_cost_usd": round(text_cost, 6),
                    "image_cost_usd": round(image_cost, 6),
                    "cost_per_section": (
                        round(total_cost / len(sections_content), 6)
                        if sections_content
                        else 0
                    ),
                    "estimated_total_tokens": total_tokens,
                    "status": "completed",
                },
                "llm_call_breakdown": {},
                "step_by_step_analysis": {},
                "cost_optimization_analysis": {
                    "current_efficiency": {
                        "model": "gpt-5-nano",
                        "input_cost_per_1m_tokens": "$0.05",
                        "output_cost_per_1m_tokens": "$0.40",
                        "average_cost_per_section": (
                            f"${round(total_cost / len(sections_content), 6)}"
                            if sections_content
                            else "$0"
                        ),
                        "total_sections_possible_with_1_dollar": (
                            int(1 / (total_cost / len(sections_content)))
                            if sections_content and total_cost > 0
                            else 0
                        ),
                    }
                },
            },
            "keyword_analysis": {
                "primary_keyword": keyword,
                "lsi_keywords": keywords.get("lsi_keywords", []),
                "longtail_keywords": keywords.get("longtail_keywords", []),
                "keyword_density": "ìì—°ìŠ¤ëŸ½ê²Œ ë°°ì¹˜ë¨",
            },
            "content_analysis": {"sections": []},
            "model_configuration": {
                "primary_model": "gpt-5-nano",
                "temperature": 1.0,
                "max_tokens": "unlimited",
                "provider": "OpenAI",
                "rag_enabled": self.rag is not None and self.rag.vs is not None,
            },
            "performance_metrics": {
                "tokens_per_second": (
                    round(total_tokens / total_duration, 1) if total_duration > 0 else 0
                ),
                "cost_per_minute": (
                    f"${round(total_cost * 60 / total_duration, 6)}"
                    if total_duration > 0
                    else "$0"
                ),
                "characters_generated": sum(
                    len(section["content"]) for section in sections_content
                ),
                "cost_per_character": (
                    f"${round(total_cost / sum(len(section['content']) for section in sections_content), 8)}"
                    if sections_content
                    else "$0"
                ),
            },
        }

        # ë‹¨ê³„ë³„ ë¶„ì„ ì¶”ê°€ (í…ìŠ¤íŠ¸)
        for i, step in enumerate(self.cost_tracker["step_details"], 1):
            report["cost_analysis"]["step_by_step_analysis"][
                f"step_{i}_{step['step']}"
            ] = {
                "timestamp": step["timestamp"],
                "duration": f"{step['duration_seconds']:.1f}ì´ˆ",
                "model_calls": 1,
                "cost": f"${step['estimated_cost_usd']:.6f}",
                "output": step["output_summary"],
                "type": "text_generation",
            }

        # ì´ë¯¸ì§€ ìƒì„± ë¶„ì„ ì¶”ê°€
        for i, img in enumerate(self.cost_tracker["image_details"], 1):
            report["cost_analysis"]["step_by_step_analysis"][
                f"image_{i}_{img['purpose']}"
            ] = {
                "timestamp": img["timestamp"],
                "duration": f"{img['duration_seconds']:.1f}ì´ˆ",
                "model_calls": 1,
                "cost": f"${img['cost_usd']:.3f}",
                "output": img["prompt"],
                "type": "image_generation",
                "model": img["model"],
                "quality": img["quality"],
                "size": img["size"],
            }

        # ì„¹ì…˜ë³„ ë¶„ì„ ì¶”ê°€
        for i, section in enumerate(sections_content):
            report["content_analysis"]["sections"].append(
                {
                    "section_number": i + 1,
                    "h2_title": section["h2_title"],
                    "character_count": len(section["content"]),
                    "estimated_tokens": len(section["content"].split()) * 1.3,
                    "h3_count": section["content"].count("### "),
                    "keywords_used": section.get("target_keywords", []),
                }
            )

        return report

    def setup_wordpress(self) -> bool:
        """ì›Œë“œí”„ë ˆìŠ¤ ì—°ê²° ì„¤ì • ë° í…ŒìŠ¤íŠ¸"""
        try:
            self.wordpress_poster = create_wordpress_poster()
            return self.wordpress_poster.test_connection()
        except Exception as e:
            print(f"ì›Œë“œí”„ë ˆìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False

    def setup_content_storage(self) -> bool:
        """ì½˜í…ì¸  ì €ì¥ì†Œ ì„¤ì •"""
        try:
            self.content_storage = create_content_storage()
            stats = self.content_storage.get_storage_stats()
            print(f"   ì½˜í…ì¸  ì €ì¥ì†Œ: {stats['total_posts']}ê°œ í¬ìŠ¤íŠ¸ ì €ì¥ë¨")

            # ë‚´ë¶€ë§í¬ ë¹Œë”ë„ í•¨ê»˜ ì„¤ì • (ì½˜í…ì¸  ì €ì¥ì†Œê°€ ìˆì„ ë•Œë§Œ)
            if self.content_storage and stats["total_posts"] > 0:
                self.internal_link_builder = create_internal_link_builder(
                    self.content_storage
                )
                print(
                    f"   ë‚´ë¶€ë§í¬ ë¹Œë”: í™œì„±í™” ({stats['total_posts']}ê°œ í¬ìŠ¤íŠ¸ ê¸°ë°˜)"
                )
            else:
                print(f"   ë‚´ë¶€ë§í¬ ë¹Œë”: ë¹„í™œì„±í™” (ì €ì¥ëœ í¬ìŠ¤íŠ¸ ì—†ìŒ)")

            return True
        except Exception as e:
            print(f"ì½˜í…ì¸  ì €ì¥ì†Œ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False

    async def upload_to_wordpress(
        self,
        title: str,
        html_content: str,
        keyword: str,
        lsi_keywords: List[str] = None,
        longtail_keywords: List[str] = None,
        images_dir: Optional[Path] = None,
    ) -> Optional[Dict[str, Any]]:
        """ì›Œë“œí”„ë ˆìŠ¤ì— ì½˜í…ì¸  ì—…ë¡œë“œ"""
        if not self.wordpress_poster:
            print("âš ï¸ ì›Œë“œí”„ë ˆìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ. setup_wordpress()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
            return None

        try:
            print("9. ì›Œë“œí”„ë ˆìŠ¤ ì—…ë¡œë“œ ì¤‘...")

            # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° HTML ì½˜í…ì¸  ë‚´ ì´ë¯¸ì§€ URL êµì²´
            if images_dir and images_dir.exists():
                html_content = self.wordpress_poster.process_images_in_content(
                    html_content, images_dir
                )

            # ì¹´í…Œê³ ë¦¬ ìë™ ì„ ë³„
            all_keywords = [keyword]
            if lsi_keywords:
                all_keywords.extend(lsi_keywords)
            if longtail_keywords:
                all_keywords.extend(longtail_keywords)

            categories = self.wordpress_poster.select_best_categories(
                title=title, content=html_content, keywords=all_keywords
            )

            # íƒœê·¸ ì„¤ì •
            tags = [keyword]  # ì£¼ìš” í‚¤ì›Œë“œë¥¼ íƒœê·¸ë¡œ

            # LSI í‚¤ì›Œë“œë¥¼ íƒœê·¸ë¡œ ì¶”ê°€ (ì²˜ìŒ 5ê°œë§Œ)
            if lsi_keywords:
                tags.extend(lsi_keywords[:5])

            # ë¡±í…Œì¼ í‚¤ì›Œë“œ ì¤‘ ì§§ì€ ê²ƒë“¤ì„ íƒœê·¸ë¡œ ì¶”ê°€ (ì²˜ìŒ 3ê°œë§Œ)
            if longtail_keywords:
                short_longtails = [lt for lt in longtail_keywords[:3] if len(lt) < 20]
                tags.extend(short_longtails)

            # ì¤‘ë³µ ì œê±°
            tags = list(set(tags))

            # ëŒ€í‘œ ì´ë¯¸ì§€ ì„¤ì • (ë©”ì¸ ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°)
            featured_image_path = None
            if images_dir:
                main_image_files = list(images_dir.glob("main_*.png"))
                if main_image_files:
                    featured_image_path = main_image_files[0]

            # ì›Œë“œí”„ë ˆìŠ¤ì— í¬ìŠ¤íŠ¸ ì—…ë¡œë“œ
            result = self.wordpress_poster.post_article(
                title=title,
                html_content=html_content,
                status="publish",  # ì¦‰ì‹œ ë°œí–‰
                category_names=categories,
                tag_names=tags,
                excerpt=f"{keyword}ì— ëŒ€í•œ ì™„ë²½í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.",
                featured_image_path=featured_image_path,
            )

            if result:
                print(f"   âœ… ì›Œë“œí”„ë ˆìŠ¤ ì—…ë¡œë“œ ì„±ê³µ!")
                print(f"   ğŸ“„ í¬ìŠ¤íŠ¸ ID: {result['id']}")
                print(f"   ğŸ”— URL: {result['url']}")
                return result
            else:
                print(f"   âŒ ì›Œë“œí”„ë ˆìŠ¤ ì—…ë¡œë“œ ì‹¤íŒ¨")
                return None

        except Exception as e:
            print(f"ì›Œë“œí”„ë ˆìŠ¤ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    async def run_complete_pipeline(
        self, keyword: str, upload_to_wp: bool = False
    ) -> Dict[str, Any]:
        """ì™„ì „í•œ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = time.time()

        print(f"í‚¤ì›Œë“œ '{keyword}'ë¡œ Enhanced RAG íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("=" * 60)

        try:
            # 1. RAG ì„¤ì •
            rag_enabled = self.setup_rag()
            print(f"1. RAG ì‹œìŠ¤í…œ: {'í™œì„±í™”' if rag_enabled else 'ë¹„í™œì„±í™”'}")

            # ì›Œë“œí”„ë ˆìŠ¤ ì„¤ì • (ì—…ë¡œë“œê°€ ìš”ì²­ëœ ê²½ìš°)
            wp_ready = False
            if upload_to_wp:
                wp_ready = self.setup_wordpress()
                print(f"   ì›Œë“œí”„ë ˆìŠ¤: {'ì—°ê²°ë¨' if wp_ready else 'ì—°ê²° ì‹¤íŒ¨'}")

            # ì½˜í…ì¸  ì €ì¥ì†Œ ì„¤ì • (í•­ìƒ í™œì„±í™”)
            storage_ready = self.setup_content_storage()

            # 2. ë‹¨ì¼ í˜¸ì¶œ: ì œëª©+í‚¤ì›Œë“œ JSON
            print("2. ì œëª©/í‚¤ì›Œë“œ ë‹¨ì¼ JSON ìƒì„± ì¤‘...")
            tk = await self.generate_title_keywords(keyword)
            print(f"   ì œëª©: {tk['title']}")

            # ì €ì¥ (STEP1)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_kw = self._safe_fragment(keyword)
            step1_file = (
                project_root / f"data/title_keywords_{safe_kw}_{timestamp}.json"
            )
            with open(step1_file, "w", encoding="utf-8") as f:
                json.dump(tk, f, ensure_ascii=False, indent=2)
            alias_step1 = project_root / f"data/OUTPUT_{safe_kw}_step1.json"
            with open(alias_step1, "w", encoding="utf-8") as f:
                json.dump(tk, f, ensure_ascii=False, indent=2)

            # 3. êµ¬ì¡° JSON (7-10ê°œ H2)
            print("3. êµ¬ì¡°(JSON) ìƒì„± ì¤‘...")
            structure = await self.generate_structure_json(
                tk["title"],
                keyword,
                tk.get("lsi_keywords", []),
                tk.get("longtail_keywords", []),
            )
            step2_file = project_root / f"data/structure_{safe_kw}_{timestamp}.json"
            with open(step2_file, "w", encoding="utf-8") as f:
                json.dump(structure, f, ensure_ascii=False, indent=2)
            alias_step2 = project_root / f"data/OUTPUT_{safe_kw}_structure.json"
            with open(alias_step2, "w", encoding="utf-8") as f:
                json.dump(structure, f, ensure_ascii=False, indent=2)

            sections = structure.get("sections", [])
            print(f"   ì„¹ì…˜ ìˆ˜: {len(sections)}ê°œ")

            # 4. ì„¹ì…˜ë³„ ë³¸ë¬¸ ìƒì„± (ì—°ì‡„ ìš”ì•½ ì‚¬ìš©)
            print("4. ì„¹ì…˜ë³„ ì½˜í…ì¸  ìƒì„± ì¤‘...")
            sections_content = []
            prev_summary = ""
            all_section_keywords = []  # ì„¹ì…˜ë³„ ì‚¬ìš©ëœ í‚¤ì›Œë“œ ì¶”ì 
            total = len(sections)

            for i, sec in enumerate(sections, 1):
                next_h2 = sections[i]["h2"] if i < total else ""
                raw, section_keywords = await self.generate_section_with_context(
                    idx=i,
                    total=total,
                    section=sec,
                    keyword=keyword,
                    title=tk["title"],
                    full_structure_json=structure,
                    prev_summary=prev_summary,
                    next_h2=next_h2,
                    lsi_keywords=tk.get("lsi_keywords", []),
                    longtail_keywords=tk.get("longtail_keywords", []),
                )

                # ì„¹ì…˜ë³„ ì‚¬ìš©ëœ í‚¤ì›Œë“œ ì €ì¥
                all_section_keywords.extend(section_keywords)

                # ëª¨ë¸ ì‘ë‹µ í›„ ì •ë¦¬: ì¤‘ë³µ H2/ì•ˆë‚´ë¬¸ ì œê±°
                content = self._sanitize_section_content(sec.get("h2", ""), raw)
                sections_content.append(
                    {
                        "h2_title": sec.get("h2", f"ì„¹ì…˜ {i}"),
                        "content": content,
                        "target_keywords": [],
                    }
                )

                # ë‹¤ìŒì„ ìœ„í•œ ìš”ì•½ ìƒì„±
                prev_summary = await self.summarize_previous(content)

            # 5. ì™¸ë¶€ë§í¬ ìƒì„± (ì´ˆê¸° ì½˜í…ì¸ ìš©)
            print("5. ì™¸ë¶€ë§í¬ ìƒì„± ì¤‘...")
            content_count = 1  # TODO: ì‹¤ì œ ì½˜í…ì¸  ìˆ˜ ì¶”ì  ì‹œìŠ¤í…œ êµ¬í˜„ ì˜ˆì •

            # ì„ì‹œ ë§ˆí¬ë‹¤ìš´ ìƒì„± (í‚¤ì›Œë“œ ì¶”ì¶œìš©)
            temp_md_content = self.create_markdown(
                tk["title"],
                keyword,
                sections_content,
                {
                    "lsi_keywords": tk.get("lsi_keywords", []),
                    "longtail_keywords": tk.get("longtail_keywords", []),
                },
                {},  # ì´ë¯¸ì§€ëŠ” ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ
                "",  # ëª©ì°¨ëŠ” ë¹ˆ ë¬¸ìì—´ (í‚¤ì›Œë“œ ì¶”ì¶œìš©ì´ë¯€ë¡œ ë¶ˆí•„ìš”)
                "",  # ìš©ì–´ ì •ë¦¬ë„ ë¹ˆ ë¬¸ìì—´ (ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ)
            )

            # ì„¹ì…˜ë³„ë¡œ ì‹¤ì œ ì‚¬ìš©ëœ í‚¤ì›Œë“œ ìˆ˜ì§‘ (ë” ì •í™•í•œ ì¶”ì )
            all_used_keywords = []

            # ë©”ì¸ í‚¤ì›Œë“œ (í•­ìƒ í¬í•¨)
            all_used_keywords.append((keyword, "ë©”ì¸"))

            # ì„¹ì…˜ë³„ í‚¤ì›Œë“œ ë¶„ë¥˜ ë° ì¶”ê°€
            lsi_used = set()
            longtail_used = set()

            for kw in all_section_keywords:
                if kw in tk.get("lsi_keywords", []):
                    lsi_used.add(kw)
                elif kw in tk.get("longtail_keywords", []):
                    longtail_used.add(kw)

            # ì‹¤ì œ ì‚¬ìš©ëœ í‚¤ì›Œë“œ ì¶”ê°€
            for kw in lsi_used:
                all_used_keywords.append((kw, "LSI"))
            for kw in longtail_used:
                all_used_keywords.append((kw, "ë¡±í…Œì¼"))

            # í‚¤ì›Œë“œ ì‚¬ìš©ëŸ‰ ì •ë¦¬ (ë°±ì—…ìš©ìœ¼ë¡œ ê¸°ì¡´ ë°©ì‹ë„ ìœ ì§€)
            used_keywords = {
                "keyword": keyword,
                "lsi_keywords": list(lsi_used),
                "longtail_keywords": list(longtail_used),
            }

            print(f"   ğŸ“Š ì‹¤ì œ ì‚¬ìš©ëœ í‚¤ì›Œë“œ: {len(all_used_keywords)}ê°œ")
            print(f"     - ë©”ì¸: 1ê°œ")
            print(
                f"     - LSI: {len(used_keywords.get('lsi_keywords', []))}ê°œ (ì „ì²´ {len(tk.get('lsi_keywords', []))}ê°œ ì¤‘)"
            )
            print(
                f"     - ë¡±í…Œì¼: {len(used_keywords.get('longtail_keywords', []))}ê°œ (ì „ì²´ {len(tk.get('longtail_keywords', []))}ê°œ ì¤‘)"
            )

            # ì‚¬ìš©ëœ í‚¤ì›Œë“œ ìƒì„¸ í‘œì‹œ
            if all_used_keywords:
                used_kw_list = [f"{kw}({kw_type})" for kw, kw_type in all_used_keywords]
                print(f"     - ì‚¬ìš©ëœ í‚¤ì›Œë“œ ëª©ë¡: {', '.join(used_kw_list)}")

            # 5-1. ë‚´ë¶€ë§í¬ ìš°ì„  ìƒì„± (ì‚¬ìš©ëœ ëª¨ë“  í‚¤ì›Œë“œ ëŒ€ìƒ)
            internal_links = []
            internal_keywords = []  # ì‹¤ì œë¡œ ë‚´ë¶€ë§í¬ì— ì‚¬ìš©ëœ í‚¤ì›Œë“œë“¤

            if self.internal_link_builder and all_used_keywords:
                print(
                    f"   ğŸ”— ë‚´ë¶€ë§í¬ ìƒì„± ì¤‘... (ì‚¬ìš© í‚¤ì›Œë“œ {len(all_used_keywords)}ê°œ ê²€ì‚¬)"
                )

                # ì„ì‹œ í¬ìŠ¤íŠ¸ ID ìƒì„±
                temp_post_id = f"temp_{int(time.time())}"

                # ëª¨ë“  ì‚¬ìš©ëœ í‚¤ì›Œë“œë¡œ ë‚´ë¶€ë§í¬ ìƒì„± ì‹œë„
                all_keywords_data = {
                    "lsi_keywords": [
                        kw for kw, kw_type in all_used_keywords if kw_type == "LSI"
                    ],
                    "longtail_keywords": [
                        kw for kw, kw_type in all_used_keywords if kw_type == "ë¡±í…Œì¼"
                    ],
                }
                main_keyword = next(
                    (kw for kw, kw_type in all_used_keywords if kw_type == "ë©”ì¸"),
                    keyword,
                )

                internal_links = self.internal_link_builder.generate_internal_links(
                    current_post_id=temp_post_id,
                    keywords_data=all_keywords_data,
                    target_keyword=main_keyword,
                    markdown_content=temp_md_content,
                    max_links=len(all_used_keywords),  # ëª¨ë“  í‚¤ì›Œë“œ ì‹œë„
                    min_similarity_score=0.3,  # ë” ì™„í™”ëœ ìœ ì‚¬ë„
                )

                # ì‹¤ì œë¡œ ë‚´ë¶€ë§í¬ì— ì‚¬ìš©ëœ í‚¤ì›Œë“œë“¤ ìˆ˜ì§‘
                internal_keywords = [
                    (link.anchor_text, "ì‚¬ìš©ë¨") for link in internal_links
                ]

                if internal_links:
                    internal_summary = (
                        self.internal_link_builder.get_internal_links_summary(
                            internal_links
                        )
                    )
                    print(f"   ğŸ“ ìƒì„±ëœ ë‚´ë¶€ë§í¬: {internal_summary['ì´_ë§í¬_ìˆ˜']}ê°œ")
                    print(f"     - í‰ê·  ìœ ì‚¬ë„: {internal_summary['í‰ê· _ìœ ì‚¬ë„']}")
                else:
                    print(f"   ğŸ“ ìƒì„±ëœ ë‚´ë¶€ë§í¬: 0ê°œ (ìœ ì‚¬í•œ í¬ìŠ¤íŠ¸ ì—†ìŒ)")
            else:
                print(f"   ğŸ“ ë‚´ë¶€ë§í¬: ê±´ë„ˆëœ€ (ì €ì¥ëœ í¬ìŠ¤íŠ¸ ì—†ìŒ)")

            # 5-2. ì™¸ë¶€ë§í¬ ìƒì„± (ë‚´ë¶€ë§í¬ì— ì‚¬ìš©ë˜ì§€ ì•Šì€ í‚¤ì›Œë“œë“¤ë¡œ)
            external_links = []
            internal_used_keywords = {link.anchor_text for link in internal_links}

            # ë‚´ë¶€ë§í¬ì— ì‚¬ìš©ë˜ì§€ ì•Šì€ í‚¤ì›Œë“œë“¤ ìˆ˜ì§‘
            remaining_keywords = [
                (kw, kw_type)
                for kw, kw_type in all_used_keywords
                if kw not in internal_used_keywords
            ]

            print(f"   ğŸŒ ì™¸ë¶€ë§í¬ ìƒì„± ì¤‘...")
            print(f"     - ë‚´ë¶€ë§í¬ ì‚¬ìš© í‚¤ì›Œë“œ: {len(internal_used_keywords)}ê°œ")
            print(f"     - ì™¸ë¶€ë§í¬ ëŒ€ìƒ í‚¤ì›Œë“œ: {len(remaining_keywords)}ê°œ")

            if remaining_keywords:
                remaining_kw_list = [
                    f"{kw}({kw_type})" for kw, kw_type in remaining_keywords
                ]
                print(f"     - ì™¸ë¶€ë§í¬ í‚¤ì›Œë“œ: {', '.join(remaining_kw_list)}")

                # ì™¸ë¶€ë§í¬ ìƒì„±
                for kw, kw_type in remaining_keywords:
                    link = self.external_link_builder.create_external_link(kw, kw_type)
                    external_links.append(link)

            if external_links:
                link_summary = self.external_link_builder.get_links_summary(
                    external_links
                )
                print(f"   ğŸ“ ìƒì„±ëœ ì™¸ë¶€ë§í¬: {link_summary['ì´_ë§í¬_ìˆ˜']}ê°œ")
            else:
                print(f"   ğŸ“ ìƒì„±ëœ ì™¸ë¶€ë§í¬: 0ê°œ (ëª¨ë“  í‚¤ì›Œë“œê°€ ë‚´ë¶€ë§í¬ì— ì‚¬ìš©ë¨)")

            # 6. ì´ë¯¸ì§€ ìƒì„± (ì œëª© + ì„¹ì…˜ë³„ 20% í™•ë¥ )
            print("6. ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
            images = {}

            # ë©”ì¸ ì´ë¯¸ì§€ (ì œëª© ê¸°ë°˜, 100% í™•ë¥ )
            print("   - ë©”ì¸ ì´ë¯¸ì§€ ìƒì„±...")
            main_prompt = f"Clean minimalist infographic diagram about '{keyword}', visual concept illustration, no text or letters, chart elements, flow diagram style, professional design, simple color scheme"
            main_image_b64 = await self.generate_image(main_prompt, "main_title")

            if main_image_b64:
                main_image_filename = f"main_{safe_kw}.png"
                main_image_path = project_root / f"data/images/{main_image_filename}"
                if self.save_image_from_base64(main_image_b64, main_image_path):
                    # ì›Œë“œí”„ë ˆìŠ¤ í˜¸í™˜ URL í˜•íƒœ (í–¥í›„ ì—…ë¡œë“œ ì‹œ êµì²´ ì˜ˆì •)
                    images["main"] = (
                        f"https://your-wordpress-site.com/wp-content/uploads/2024/images/{main_image_filename}"
                    )
                    print(f"     âœ… ë©”ì¸ ì´ë¯¸ì§€ ì €ì¥: {main_image_filename}")

            # ì„¹ì…˜ë³„ ì´ë¯¸ì§€ (20% í™•ë¥ )
            for i, section in enumerate(sections_content):
                if random.random() < 0.2:  # 20% í™•ë¥ 
                    print(f"   - ì„¹ì…˜ {i+1} ì´ë¯¸ì§€ ìƒì„±: {section['h2_title']}")
                    section_prompt = f"Simple visual diagram for '{section['h2_title']}' concept related to {keyword}, no text or words, minimalist chart design, geometric shapes, clean infographic elements, conceptual illustration"
                    section_image_b64 = await self.generate_image(
                        section_prompt, f"section_{i+1}"
                    )

                    if section_image_b64:
                        section_image_filename = f"section_{i+1}_{safe_kw}.png"
                        section_image_path = (
                            project_root / f"data/images/{section_image_filename}"
                        )
                        if self.save_image_from_base64(
                            section_image_b64, section_image_path
                        ):
                            # ì›Œë“œí”„ë ˆìŠ¤ í˜¸í™˜ URL í˜•íƒœ (í–¥í›„ ì—…ë¡œë“œ ì‹œ êµì²´ ì˜ˆì •)
                            images[f"section_{i+1}"] = (
                                f"https://your-wordpress-site.com/wp-content/uploads/2024/images/{section_image_filename}"
                            )
                            print(f"     âœ… ì„¹ì…˜ ì´ë¯¸ì§€ ì €ì¥: {section_image_filename}")

            total_duration = time.time() - start_time

            # 7. ëª©ì°¨ ë° ìš©ì–´ ì •ë¦¬ ìƒì„±
            print("7. ëª©ì°¨ ë° ìš©ì–´ ì •ë¦¬ ìƒì„± ì¤‘...")

            # ëª©ì°¨ ìƒì„± (LLM í˜¸ì¶œ ì—†ì´)
            table_of_contents = self.generate_table_of_contents(sections_content)
            print("   âœ… ëª©ì°¨ ìƒì„± ì™„ë£Œ")

            # ì „ì²´ ì½˜í…ì¸  ì¡°í•© (ìš©ì–´ ì¶”ì¶œìš©)
            full_content = "\n".join([sec["content"] for sec in sections_content])

            # ìš©ì–´ ì¶”ì¶œ ë° ì„¤ëª… ìƒì„± (LLM í˜¸ì¶œ)
            terms_section = await self.extract_and_explain_terms(full_content, keyword)
            print("   âœ… ìš©ì–´ ì •ë¦¬ ìƒì„± ì™„ë£Œ")

            # 8. íŒŒì¼ ìƒì„± (ë³„ì¹­ í¬í•¨)
            print("8. íŒŒì¼ ìƒì„± ì¤‘...")
            timestamp2 = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_kw2 = self._safe_fragment(keyword)

            # MD (ëª©ì°¨ + ìš©ì–´ ì •ë¦¬ + ì´ë¯¸ì§€ + ì™¸ë¶€ë§í¬ í¬í•¨)
            md_content = self.create_markdown(
                tk["title"],
                keyword,
                sections_content,
                {
                    "lsi_keywords": tk.get("lsi_keywords", []),
                    "longtail_keywords": tk.get("longtail_keywords", []),
                },
                images,  # ì´ë¯¸ì§€ ì •ë³´ ì „ë‹¬
                table_of_contents,  # ëª©ì°¨ ì¶”ê°€
                terms_section,  # ìš©ì–´ ì •ë¦¬ ì¶”ê°€
            )

            # ì™¸ë¶€ë§í¬ ì‚½ì… ì „ì— ì›ë³¸ ë§í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°±ì—…
            original_external_links = external_links.copy()

            # ì™¸ë¶€ë§í¬ ì‚½ì… (ë§í¬ ë¦¬ìŠ¤íŠ¸ê°€ ìˆ˜ì •ë  ìˆ˜ ìˆìŒ)
            md_content = self.external_link_builder.insert_links_into_markdown(
                md_content, external_links
            )

            # ë‚´ë¶€ë§í¬ ì‚½ì… (ì™¸ë¶€ë§í¬ ì‚½ì… í›„)
            if internal_links:
                print("   ğŸ”— ë‚´ë¶€ë§í¬ ë§ˆí¬ë‹¤ìš´ì— ì‚½ì… ì¤‘...")
                md_content = (
                    self.internal_link_builder.insert_internal_links_into_markdown(
                        md_content, internal_links
                    )
                )

            # ì‹¤ì œ ì ìš©ëœ ë§í¬ ìˆ˜ ì¬ê³„ì‚° (ì›ë³¸ ë§í¬ ë¦¬ìŠ¤íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ ì—ì„œ í™•ì¸)
            applied_links = []
            unused_links = []

            for link in original_external_links:
                # ë§ˆí¬ë‹¤ìš´ ë§í¬ íŒ¨í„´ì´ ì¡´ì¬í•˜ëŠ”ì§€ ì²´í¬: [ì•µì»¤í…ìŠ¤íŠ¸](ì–´ë–¤URLì´ë“ )
                link_pattern = f"[{link.anchor_text}]("
                if link_pattern in md_content:
                    applied_links.append(link)
                else:
                    unused_links.append(link)
            md_file = project_root / f"data/blog_{safe_kw2}_{timestamp2}.md"
            md_file.parent.mkdir(exist_ok=True)
            with open(md_file, "w", encoding="utf-8") as f:
                f.write(md_content)
            alias_md = project_root / f"data/OUTPUT_{safe_kw2}.md"
            with open(alias_md, "w", encoding="utf-8") as f:
                f.write(md_content)

            # HTML (SimpleHTMLConverter ì‚¬ìš©)
            from src.generators.html.simple_html_converter import SimpleHTMLConverter

            converter = SimpleHTMLConverter()
            html_content = converter.convert_markdown_to_html(md_content)
            html_file = project_root / f"data/blog_{safe_kw2}_{timestamp2}_final.html"
            with open(html_file, "w", encoding="utf-8") as f:
                f.write(html_content)
            alias_html = project_root / f"data/OUTPUT_{safe_kw2}.html"
            with open(alias_html, "w", encoding="utf-8") as f:
                f.write(html_content)

            # ë¹„ìš© ë¦¬í¬íŠ¸ (ì™¸ë¶€ë§í¬ ì •ë³´ í¬í•¨)
            cost_report = self.create_cost_analysis_report(
                keyword,
                tk["title"],
                {
                    "lsi_keywords": tk.get("lsi_keywords", []),
                    "longtail_keywords": tk.get("longtail_keywords", []),
                },
                sections_content,
                total_duration,
            )

            # ë§í¬ ë¶„ì„ ì •ë³´ ì¶”ê°€ (ì™¸ë¶€ë§í¬ + ë‚´ë¶€ë§í¬)
            cost_report["link_analysis"] = {
                "external_links_generated": len(applied_links),
                "internal_links_generated": len(internal_links),
                "external_link_summary": {
                    "ì´_ë§í¬_ìˆ˜": len(applied_links),
                    "ì™¸ë¶€ë§í¬_ìˆ˜": len(
                        [l for l in applied_links if l.platform != "í™ˆí˜ì´ì§€"]
                    ),
                    "í™ˆí˜ì´ì§€_ë§í¬_ìˆ˜": len(
                        [l for l in applied_links if l.platform == "í™ˆí˜ì´ì§€"]
                    ),
                    "í”Œë«í¼ë³„": {},
                },
                "internal_link_summary": (
                    self.internal_link_builder.get_internal_links_summary(
                        internal_links
                    )
                    if internal_links
                    else {}
                ),
                "external_link_details": [
                    {
                        "anchor_text": link.anchor_text,
                        "url": link.url,
                        "platform": link.platform,
                        "keyword_type": link.keyword_type,
                    }
                    for link in applied_links
                ],
                "internal_link_details": [
                    {
                        "anchor_text": link.anchor_text,
                        "target_url": link.target_url,
                        "target_title": link.target_title,
                        "similarity_score": link.similarity_score,
                        "keyword_type": link.keyword_type,
                    }
                    for link in internal_links
                ],
                "unused_external_links": [
                    {
                        "anchor_text": link.anchor_text,
                        "url": link.url,
                        "platform": link.platform,
                        "keyword_type": link.keyword_type,
                        "reason": "í‚¤ì›Œë“œë¥¼ ë³¸ë¬¸ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ",
                    }
                    for link in unused_links
                ],
            }

            # í”Œë«í¼ë³„ í†µê³„ ê³„ì‚°
            for link in applied_links:
                platform = link.platform
                if (
                    platform
                    not in cost_report["link_analysis"]["external_link_summary"][
                        "í”Œë«í¼ë³„"
                    ]
                ):
                    cost_report["link_analysis"]["external_link_summary"]["í”Œë«í¼ë³„"][
                        platform
                    ] = 0
                cost_report["link_analysis"]["external_link_summary"]["í”Œë«í¼ë³„"][
                    platform
                ] += 1
            json_file = (
                project_root / f"data/cost_analysis_report_{safe_kw2}_{timestamp2}.json"
            )
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(cost_report, f, ensure_ascii=False, indent=2)
            alias_json = project_root / f"data/OUTPUT_{safe_kw2}.json"
            with open(alias_json, "w", encoding="utf-8") as f:
                json.dump(cost_report, f, ensure_ascii=False, indent=2)

            print(f"\níŒŒì¼ ìƒì„± ì™„ë£Œ:")
            print(f"   - Step1: {step1_file.name} / ë³„ì¹­: {alias_step1.name}")
            print(f"   - Step2: {step2_file.name} / ë³„ì¹­: {alias_step2.name}")
            print(f"   - Markdown: {md_file.name} / ë³„ì¹­: {alias_md.name}")
            print(f"   - HTML: {html_file.name} / ë³„ì¹­: {alias_html.name}")
            print(f"   - Cost Report: {json_file.name} / ë³„ì¹­: {alias_json.name}")

            # ì›Œë“œí”„ë ˆìŠ¤ ì—…ë¡œë“œ (ìš”ì²­ëœ ê²½ìš°)
            wp_result = None
            if upload_to_wp and wp_ready:
                wp_result = await self.upload_to_wordpress(
                    title=tk["title"],
                    html_content=html_content,
                    keyword=keyword,
                    lsi_keywords=tk.get("lsi_keywords", []),
                    longtail_keywords=tk.get("longtail_keywords", []),
                    images_dir=project_root / "data/images",
                )

                # ì›Œë“œí”„ë ˆìŠ¤ ì—…ë¡œë“œ í›„ ì²˜ë¦¬
                if wp_result:
                    cost_report["wordpress_upload"] = {
                        "success": True,
                        "post_id": wp_result["id"],
                        "post_url": wp_result["url"],
                        "upload_date": wp_result["date"],
                    }

                    # ì›Œë“œí”„ë ˆìŠ¤ ì—…ë¡œë“œ ì„±ê³µ ì‹œ FAISSì—ë„ ì €ì¥
                    if storage_ready and self.content_storage:
                        print("   ğŸ“š FAISS ë²¡í„° ì €ì¥ì†Œì— ì½˜í…ì¸  ì €ì¥ ì¤‘...")

                        # ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (ë§í¬, ì´ë¯¸ì§€ ë“± ì œê±°)
                        import re

                        clean_content = re.sub(
                            r"!\[.*?\]\(.*?\)", "", md_content
                        )  # ì´ë¯¸ì§€ ì œê±°
                        clean_content = re.sub(
                            r"\[([^\]]+)\]\([^)]+\)", r"\1", clean_content
                        )  # ë§í¬ í…ìŠ¤íŠ¸ë§Œ ìœ ì§€
                        clean_content = re.sub(
                            r"#+\s*", "", clean_content
                        )  # í—¤ë”© ë§ˆí¬ì—… ì œê±°
                        clean_content = re.sub(
                            r"\*\*([^*]+)\*\*", r"\1", clean_content
                        )  # ë³¼ë“œ ì œê±°
                        clean_content = re.sub(
                            r"\n\s*\n", "\n\n", clean_content
                        )  # ë¹ˆ ì¤„ ì •ë¦¬

                        storage_success = self.content_storage.store_wordpress_post(
                            post_data=wp_result,
                            content=clean_content.strip(),
                            keyword=keyword,
                            lsi_keywords=tk.get("lsi_keywords", []),
                            longtail_keywords=tk.get("longtail_keywords", []),
                            categories=["ë¸”ë¡œê·¸", "SEO"],
                        )

                        if storage_success:
                            print("   âœ… FAISS ë²¡í„° ì €ì¥ì†Œì— ì €ì¥ ì™„ë£Œ")
                            cost_report["content_storage"] = {
                                "success": True,
                                "stored_at": datetime.now().isoformat(),
                            }
                        else:
                            print("   âŒ FAISS ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì‹¤íŒ¨")
                            cost_report["content_storage"] = {
                                "success": False,
                                "error": "ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
                            }

            # ì›Œë“œí”„ë ˆìŠ¤ ì—…ë¡œë“œ ì—†ì´ë„ ì½˜í…ì¸  ì €ì¥ (ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©)
            elif not upload_to_wp and storage_ready and self.content_storage:
                print("9. FAISS ë²¡í„° ì €ì¥ì†Œì— ì½˜í…ì¸  ì €ì¥ ì¤‘... (ë¡œì»¬)")

                # ê°€ìƒì˜ í¬ìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©)
                fake_post_data = {
                    "id": f"local_{int(time.time())}",
                    "title": tk["title"],
                    "url": f"local://blog/{safe_kw2}",
                    "date": datetime.now().isoformat(),
                }

                # ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                import re

                clean_content = re.sub(r"!\[.*?\]\(.*?\)", "", md_content)
                clean_content = re.sub(r"\[([^\]]+)\]\([^)]+\)", r"\1", clean_content)
                clean_content = re.sub(r"#+\s*", "", clean_content)
                clean_content = re.sub(r"\*\*([^*]+)\*\*", r"\1", clean_content)
                clean_content = re.sub(r"\n\s*\n", "\n\n", clean_content)

                storage_success = self.content_storage.store_wordpress_post(
                    post_data=fake_post_data,
                    content=clean_content.strip(),
                    keyword=keyword,
                    lsi_keywords=tk.get("lsi_keywords", []),
                    longtail_keywords=tk.get("longtail_keywords", []),
                    categories=["ë¸”ë¡œê·¸", "SEO"],
                )

                if storage_success:
                    print("   âœ… FAISS ë²¡í„° ì €ì¥ì†Œì— ì €ì¥ ì™„ë£Œ (ë¡œì»¬)")
                    cost_report["content_storage"] = {
                        "success": True,
                        "stored_at": datetime.now().isoformat(),
                        "type": "local",
                    }
                else:
                    print("   âŒ FAISS ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì‹¤íŒ¨")
                    cost_report["content_storage"] = {
                        "success": False,
                        "error": "ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
                    }

            # ì—…ë°ì´íŠ¸ëœ ë¹„ìš© ë¦¬í¬íŠ¸ ì €ì¥
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(cost_report, f, ensure_ascii=False, indent=2)
            with open(alias_json, "w", encoding="utf-8") as f:
                json.dump(cost_report, f, ensure_ascii=False, indent=2)

            print("\n" + "=" * 60)
            print("Enhanced RAG íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            print("=" * 60)
            print(f"í‚¤ì›Œë“œ: {keyword}")
            print(f"ì œëª©: {tk['title']}")
            print(f"ëª¨ë¸: gpt-5-nano + gpt-image-1 (temperature=1.0)")
            print(f"RAG í™œì„±í™”: {'ì˜ˆ' if rag_enabled else 'ì•„ë‹ˆì˜¤'}")
            print(f"ìƒì„± ì‹œê°„: {total_duration:.1f}ì´ˆ")
            print(f"ì„¹ì…˜ ìˆ˜: {len(sections_content)}ê°œ")
            print(f"ìƒì„±ëœ ì´ë¯¸ì§€: {len(images)}ê°œ")
            print(f"ìƒì„±ëœ ì™¸ë¶€ë§í¬: {len(applied_links)}ê°œ")
            print(f"ìƒì„±ëœ ë‚´ë¶€ë§í¬: {len(internal_links)}ê°œ")
            if unused_links:
                print(
                    f"   âš ï¸  ë¯¸ì‚¬ìš© ì™¸ë¶€ë§í¬: {len(unused_links)}ê°œ (í‚¤ì›Œë“œë¥¼ ë³¸ë¬¸ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ)"
                )
            print(
                f"ì´ ì½˜í…ì¸  ê¸¸ì´: {sum(len(s['content']) for s in sections_content):,}ì"
            )
            # ì´ë¯¸ì§€ ë¹„ìš© í‘œì‹œ
            if len(images) > 0:
                total_image_cost = sum(
                    img["cost_usd"] for img in self.cost_tracker["image_details"]
                )
                print(f"ì´ë¯¸ì§€ ìƒì„± ë¹„ìš©: ${total_image_cost:.3f}")
                print(f"ìƒì„±ëœ ì´ë¯¸ì§€ íŒŒì¼:")
                for key, filename in images.items():
                    print(f"  - {key}: {filename}")

            # ì™¸ë¶€ë§í¬ ì •ë³´ í‘œì‹œ
            if len(applied_links) > 0:
                print(f"ì‹¤ì œ ì ìš©ëœ ì™¸ë¶€ë§í¬:")
                for link in applied_links:
                    print(f"  - {link.anchor_text} â†’ {link.platform}")

            if len(unused_links) > 0:
                print(f"ë¯¸ì‚¬ìš© ì™¸ë¶€ë§í¬:")
                for link in unused_links:
                    print(
                        f"  - {link.anchor_text} â†’ {link.platform} (ë³¸ë¬¸ì—ì„œ í‚¤ì›Œë“œ ì—†ìŒ)"
                    )

            # ë‚´ë¶€ë§í¬ ì •ë³´ í‘œì‹œ
            if len(internal_links) > 0:
                print(f"ì‹¤ì œ ì ìš©ëœ ë‚´ë¶€ë§í¬:")
                for link in internal_links:
                    print(
                        f"  - {link.anchor_text} â†’ {link.target_title} (ìœ ì‚¬ë„: {link.similarity_score:.3f})"
                    )

            # ì›Œë“œí”„ë ˆìŠ¤ ì—…ë¡œë“œ ê²°ê³¼ í‘œì‹œ
            if upload_to_wp:
                if wp_result:
                    print(f"\nğŸš€ ì›Œë“œí”„ë ˆìŠ¤ ì—…ë¡œë“œ ì™„ë£Œ:")
                    print(f"   ğŸ“„ í¬ìŠ¤íŠ¸ ID: {wp_result['id']}")
                    print(f"   ğŸ”— URL: {wp_result['url']}")
                elif wp_ready:
                    print(f"\nâŒ ì›Œë“œí”„ë ˆìŠ¤ ì—…ë¡œë“œ ì‹¤íŒ¨")
                else:
                    print(f"\nâš ï¸ ì›Œë“œí”„ë ˆìŠ¤ ì—°ê²° ì‹¤íŒ¨ë¡œ ì—…ë¡œë“œ ê±´ë„ˆëœ€")

            # ì´ë¯¸ì§€ í´ë” ì •ë¦¬
            print("\n10. ì´ë¯¸ì§€ í´ë” ì •ë¦¬ ì¤‘...")
            self.cleanup_images_folder()

            result_data = {
                "success": True,
                "files": {
                    "step1": str(alias_step1),
                    "step2": str(alias_step2),
                    "markdown": str(alias_md),
                    "html": str(alias_html),
                    "cost_report": str(alias_json),
                },
            }

            # ì›Œë“œí”„ë ˆìŠ¤ ì—…ë¡œë“œ ê²°ê³¼ ì¶”ê°€
            if wp_result:
                result_data["wordpress"] = {
                    "post_id": wp_result["id"],
                    "post_url": wp_result["url"],
                    "upload_date": wp_result["date"],
                }

            return result_data

        except Exception as e:
            print(f"\nì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback

            traceback.print_exc()
            return {"success": False, "error": str(e)}


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Enhanced RAG Pipeline for SEO Blog Generation"
    )
    parser.add_argument(
        "keyword", nargs="?", default="ë¶€ìë˜ëŠ” ìŠµê´€", help="íƒ€ê²Ÿ í‚¤ì›Œë“œ"
    )
    parser.add_argument(
        "--wp", "--wordpress", action="store_true", help="ì›Œë“œí”„ë ˆìŠ¤ì— ìë™ ì—…ë¡œë“œ"
    )
    parser.add_argument(
        "--no-wp", action="store_true", help="ì›Œë“œí”„ë ˆìŠ¤ ì—…ë¡œë“œ ê±´ë„ˆë›°ê¸° (ê¸°ë³¸ê°’)"
    )

    args = parser.parse_args()

    # ì›Œë“œí”„ë ˆìŠ¤ ì—…ë¡œë“œ ì—¬ë¶€ ê²°ì •
    upload_to_wp = args.wp and not args.no_wp

    if upload_to_wp:
        print("ğŸš€ ì›Œë“œí”„ë ˆìŠ¤ ìë™ ì—…ë¡œë“œ ëª¨ë“œ")
    else:
        print("ğŸ“ íŒŒì¼ ìƒì„±ë§Œ ìˆ˜í–‰ (ì›Œë“œí”„ë ˆìŠ¤ ì—…ë¡œë“œ ì•ˆí•¨)")

    pipeline = EnhancedRAGPipeline()
    result = await pipeline.run_complete_pipeline(
        args.keyword, upload_to_wp=upload_to_wp
    )

    if result["success"]:
        print(f"\nâœ… íŒŒì´í”„ë¼ì¸ ì„±ê³µ! ìƒì„±ëœ íŒŒì¼ë“¤ì„ í™•ì¸í•˜ì„¸ìš”.")
        if "wordpress" in result:
            print(f"ğŸŒ ì›Œë“œí”„ë ˆìŠ¤ í¬ìŠ¤íŠ¸: {result['wordpress']['post_url']}")
    else:
        print(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {result['error']}")


if __name__ == "__main__":
    asyncio.run(main())
